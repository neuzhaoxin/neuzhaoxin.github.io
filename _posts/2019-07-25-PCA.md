---
layout: post
title:  "Principal Component Analysis(PCA)"
date:   2019-07-25 15:06:32
categories: 机器学习
tags: 算法 降维 数据处理
---

* content
{:toc}

主要介绍了PCA的原理。




## 1、核心思想

PCA (Principal Component Analysis，主成分分析），是目前最常用的数据降维方法之一，主要思路是将n维的数据投影到k(n>k)维空间超平面（直线的高维推广）上面去，使得各个样本点到超平面的投影距离最小（欧式距离）且方差最大。

## 2、几何意义

数据原本有n个特征，也就是在原坐标系下需要用n个坐标轴来表示，现在我们要选出k个坐标轴，这k个坐标轴有如下特点：

1、第一个新坐标轴选择的是原始数据中方差最大的方向

2、第二个新坐标轴选取的是与第一个坐标轴正交且具有最大方差的方向

3、第三个新坐标轴······

依次类推，我们可以取到这样的 k 个坐标轴。

## 3、理论基础

我们需要在降维之后保留原有信息，以上做法能不能保留原有信息呢？如果能的话，为什么？这就要借助最大方差理论了。

信号处理中认为信号（Signal）具有较大的方差，噪声（Noise）有较小的方差，信噪比（信号与噪声的方差比）越大越好。因此我们认为，最好的 k 维特征是将 n 维样本点转换为 k 维后，每一维上的样本方差都很大。

那么，我们如何得到这些包含最大方差的主成分方向呢？

事实上，通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。

由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法。

## 4、相关计算

均值：<a href="https://www.codecogs.com/eqnedit.php?latex=\overline{x}&space;=&space;\frac{1}{n}\sum_{i=1}^{n}x_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\overline{x}&space;=&space;\frac{1}{n}\sum_{i=1}^{n}x_{i}" title="\overline{x} = \frac{1}{n}\sum_{i=1}^{n}x_{i}" /></a>

标准差：<a href="https://www.codecogs.com/eqnedit.php?latex=s&space;=&space;\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?s&space;=&space;\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}" title="s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}" /></a>

方差：<a href="https://www.codecogs.com/eqnedit.php?latex=s^{2}&space;=&space;\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?s^{2}&space;=&space;\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}" title="s^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}" /></a>

协方差：<a href="https://www.codecogs.com/eqnedit.php?latex=cov(x,y)&space;=&space;\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?cov(x,y)&space;=&space;\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})" title="cov(x,y) = \frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})" /></a>

协方差矩阵：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/PCA/协方差矩阵.jpg)

## 5、PCA实现步骤

1、把原始数据中每个样本用一个向量表示，然后把所有样本组合起来构成一个矩阵。为了避免样本的单位的影响，样本集需要标准化（一般都是去均值化）。

2、求该矩阵的协方差矩阵。

3、求步骤2中得到的协方差矩阵的特征值和特征向量。

4、PCA 可以得到 n 个主成分，但是，由于各个主成分的方差是递减的，包含的信息量也是递减的，所以实际分析时，一般是不选取 n 个主成分，而是根据各个主成分累计贡献率的大小取前 k 个主成分。这里的贡献率就是指某个主成分的方差占全部方差的比重，实际也就是某个特征值占全部特征值合计的比重。贡献率越大，说明该主成分所包含的原始变量的信息越强。

5、主成分个数 k 的选取，主要是根据主成分的累计贡献率 来决定的，即一般要求累计贡献率达到85%以上，这样才能保证综合变量能够包含原始数据的绝大多数信息。

注：主成分是由原特征投影组成的，但是可能会存在反方向的投影，因此PCA的物理意义在这时候不太清晰，这也是PCA的一个小缺点。


## 6、PCA实例

1、数据如下：

|特征|房价（百万元）|面积（百平米）|
|:-----:|:------:|:------:|
|a|10|9|
|b|2|3|
|c|1|2|
|d|7|6.5|
|e|3|2.5|

2、去中心化：

房价的均值：4.6 面积的均值：4.6

|特征|房价（百万元）|面积（百平米）|
|:-----:|:------:|:------:|
|a|5.4|4.4|
|b|-2.6|-1.6|
|c|-3.6|-2.6|
|d|2.4|1.9|
|e|-1.6|-2.1|

得到两个向量：

<a href="https://www.codecogs.com/eqnedit.php?latex=x&space;=&space;\begin{Bmatrix}&space;5.4\\&space;-2.6\\&space;-3.6\\&space;2.4\\&space;-1.6&space;\end{Bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x&space;=&space;\begin{Bmatrix}&space;5.4\\&space;-2.6\\&space;-3.6\\&space;2.4\\&space;-1.6&space;\end{Bmatrix}" title="x = \begin{Bmatrix} 5.4\\ -2.6\\ -3.6\\ 2.4\\ -1.6 \end{Bmatrix}" /></a>                         <a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\begin{Bmatrix}&space;4.4\\&space;-1.6\\&space;-2.6\\&space;1.9\\&space;-2.1&space;\end{Bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;\begin{Bmatrix}&space;4.4\\&space;-1.6\\&space;-2.6\\&space;1.9\\&space;-2.1&space;\end{Bmatrix}" title="y = \begin{Bmatrix} 4.4\\ -1.6\\ -2.6\\ 1.9\\ -2.1 \end{Bmatrix}" /></a>

2、求协方差矩阵

<a href="https://www.codecogs.com/eqnedit.php?latex=Q&space;=&space;\begin{Bmatrix}&space;14.3&space;&11.3&space;\\&space;11.3&space;&9.175&space;\end{Bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Q&space;=&space;\begin{Bmatrix}&space;14.3&space;&11.3&space;\\&space;11.3&space;&9.175&space;\end{Bmatrix}" title="Q = \begin{Bmatrix} 14.3 &11.3 \\ 11.3 &9.175 \end{Bmatrix}" /></a>

3、协方差矩阵的特征值和特征向量

<a href="https://www.codecogs.com/eqnedit.php?latex=e_{1}&space;=&space;\begin{Bmatrix}&space;0.624&space;\\&space;-0.7814&space;\end{Bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?e_{1}&space;=&space;\begin{Bmatrix}&space;0.624&space;\\&space;-0.7814&space;\end{Bmatrix}" title="e_{1} = \begin{Bmatrix} 0.624 \\ -0.7814 \end{Bmatrix}" /></a>

对应特征值为：0.1506

<a href="https://www.codecogs.com/eqnedit.php?latex=e_{2}&space;=&space;\begin{Bmatrix}&space;-0.7814&space;\\&space;-0.624&space;\end{Bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?e_{2}&space;=&space;\begin{Bmatrix}&space;-0.7814&space;\\&space;-0.624&space;\end{Bmatrix}" title="e_{2} = \begin{Bmatrix} -0.7814 \\ -0.624 \end{Bmatrix}" /></a>

对应特征值为：23.3244

4、计算贡献率

e1:0.1506/(0.1506+23.3244) = 0.6415

e2:23.3244/(0.1506+23.3244) = 99.3585

5、计算主元

对应e2的主元：

<a href="https://www.codecogs.com/eqnedit.php?latex=a&space;=a\cdot&space;e_{2}=&space;\bigl(\begin{smallmatrix}&space;5.4&space;&4.4&space;\end{smallmatrix}\bigr)\cdot&space;\binom{-0.7814}{-0.624}=-6.96516" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a&space;=a\cdot&space;e_{2}=&space;\bigl(\begin{smallmatrix}&space;5.4&space;&4.4&space;\end{smallmatrix}\bigr)\cdot&space;\binom{-0.7814}{-0.624}=-6.96516" title="a =a\cdot e_{2}= \bigl(\begin{smallmatrix} 5.4 &4.4 \end{smallmatrix}\bigr)\cdot \binom{-0.7814}{-0.624}=-6.96516" /></a>

b=b·e2 = 3.03004

c=c·e2 = 4.43544

d=d·e2 = -3.06096

e=e·e2 = 2.56064

因此，主元1为：

<a href="https://www.codecogs.com/eqnedit.php?latex=\begin{Bmatrix}&space;-6.97\\&space;3.03\\&space;4.44\\&space;-3.06\\&space;2.56&space;\end{Bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\begin{Bmatrix}&space;-6.97\\&space;3.03\\&space;4.44\\&space;-3.06\\&space;2.56&space;\end{Bmatrix}" title="\begin{Bmatrix} -6.97\\ 3.03\\ 4.44\\ -3.06\\ 2.56 \end{Bmatrix}" /></a>

同理，可求得主元2.（基本没用，贡献太小）

## 7、PCA的MATLAB函数

1、pcacov

运用协方差矩阵或者相关系数矩阵进行主成分分析，其调用格式如下：

COEFF = pcacov(V)

[COEFF,latent] = pcacov(V)

[COEFF,latent,explained] = pcacov(V)

其中：

V：总体或样本的协方差矩阵或者相关系数矩阵。

COEFF：是 k 个主成分的系数矩阵，他的第 i 列是第 i 个主成分的系数向量。

latent：是 k 个主成分的方差构成的列向量，即 V 的 p 个特征值（从大到小）构成的向量（特征向量）。

explained：k 个主成分的贡献率向量，每个主成份的方差在观测量总方差中所占的百分数向量。

2、princomp

运用样本数据矩阵进行主成分分析，其调用格式如下：

[COEFF,SCORE] = princomp(X)

[COEFF,SCORE,latent] = princomp(X)

[COEFF,SCORE,latent,tsquare] = princomp(X)

[...] = princomp(X,'econ')

其中：

X：样本数据矩阵，输入的参数 X 是 n 行 p 列的矩阵，每一行对应一个观测样品，每一列对应一个变量。

COEFF：主成份系数矩阵。

SCORE：主成份的得分矩阵。

latent：样本协方差矩阵特征值的向量。k 个特征值构成的列向量，其中特征值按降序排列。

tsquare：包含 k 个元素的列向量，是每个数据点的 HotellingT2（霍特林）统计量。描述了第 i 个观测与样本观测矩阵之间的距离，可用来寻找远离中心的极端数据。

3、pca

coeff = pca(X)
coeff = pca(X,Name,Value)
[coeff,score,latent] = pca(___)
[coeff,score,latent,tsquared] = pca(___)
[coeff,score,latent,tsquared,explained,mu] = pca(___)

X :数据集 假设n个样本, 每个样本p维,则 X是n-by-p的matrix

latent: 主成分方差 也就是各特征向量对应的特征值,从大到小进行排列

explained : 每一个主成分所贡献的比例,可以更直观的选择所需要降维的维数了,不用再用特征值去求了

mu: X 按列的均值,当前仅当 'Centered'置于'true'(默认值)时才会返回此变量

## 参考文献

https://www.sohu.com/a/212139315_505915

https://www.jianshu.com/p/8d68911d569e

https://www.cnblogs.com/lutaitou/p/5535171.html

https://blog.csdn.net/program_developer/article/details/80632779