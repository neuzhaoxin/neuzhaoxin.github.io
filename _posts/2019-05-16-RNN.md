---
layout: post
title:  "循环神经网络（原理）"
date:   2019-05-16 8:47:26
categories: 机器学习
tags: 神经网络 RNN
---

* content
{:toc}

主要介绍一种输出和模型间有反馈的神经网络：循环神经网络(Recurrent Neural Networks ，以下简称RNN)，它广泛的用于时间序列数据，例如：自然语言处理中的语音识别、手写书别以及机器翻译等领域。





## 1、什么是RNN

RNN（Recurrent Neural Network）是一类用于处理序列数据的神经网络。既然如此，那什么是序列数据呢？

时间序列数据（time series data）是在不同时间上收集到的数据，这类数据是按时间顺序收集到的，用于所描述现象随时间变化的情况，这类数据反映了某一事物、现象等随时间的变化状态或程度。（来自百度百科）

这是时间序列数据，而序列数据包括但不仅限于时间序列数据，总而言之，后面的数据会受到前面数据的影响。

## 2、RNN模型

RNN模型有很多变形，想了解更多可以看[这里](https://blog.csdn.net/qq_16234613/article/details/79476763),本篇文章介绍的仅是标准型，其结构如下。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/RNN/RNN模型.jpg)

上图左边是没有展开的RNN模型，如果按时间序列展开，则是上图中的右边部分。我们重点观察右边部分的图。

我们从基础的神经网络中知道，神经网络包含输入层、隐层、输出层，通过激活函数控制输出，层与层之间通过权值连接。激活函数是事先确定好的，那么神经网络模型通过训练“学“到的东西就蕴含在“权值”、“阈值”中。基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。在上图中：

1）<a href="https://www.codecogs.com/eqnedit.php?latex=x^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x^{(t)}" title="x^{(t)}" /></a>
为t时刻训练样本的输入；

2）<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t)}" title="h^{(t)}" /></a>
为t时刻模型的隐层状态，该变量由
<a href="https://www.codecogs.com/eqnedit.php?latex=x^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x^{(t)}" title="x^{(t)}" /></a>
和
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t-1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t-1)}" title="h^{(t-1)}" /></a>
共同决定；

3）<a href="https://www.codecogs.com/eqnedit.php?latex=o^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o^{(t)}" title="o^{(t)}" /></a>
为t时刻模型的输出，该变量仅由
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t)}" title="h^{(t)}" /></a>
决定；

4）<a href="https://www.codecogs.com/eqnedit.php?latex=L^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L^{(t)}" title="L^{(t)}" /></a>
为t时刻模型的损失函数；

5）<a href="https://www.codecogs.com/eqnedit.php?latex=y^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y^{(t)}" title="y^{(t)}" /></a>
为t时刻模型训练集的输出；

6）U、V、W是模型权值，需要注意的是，在模型中权值是共享的，也就是所有的U均相同，所有的V均相同，所有的W均相同。

## 3、前向传播

有了以上的模型，前向传播算法很容易就可以得出。

由上述第（2）条特征，可以计算得出任意t时刻，

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=h(t)=\sigma&space;(Ux^{(t)}&plus;Wh^{(t-1)}&plus;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(t)=\sigma&space;(Ux^{(t)}&plus;Wh^{(t-1)}&plus;b)" title="h(t)=\sigma (Ux^{(t)}+Wh^{(t-1)}+b)" /></a>　　　　（公式1）

其中，
<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma" title="\sigma" /></a>
为激活函数，一般为
<a href="https://www.codecogs.com/eqnedit.php?latex=tanh" target="_blank"><img src="https://latex.codecogs.com/gif.latex?tanh" title="tanh" /></a>
函数，b为线性偏置，相当于隐层阈值。

得到
<a href="https://www.codecogs.com/eqnedit.php?latex=h(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(t)" title="h(t)" /></a>
之后，可以计算得出
<a href="https://www.codecogs.com/eqnedit.php?latex=o^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o^{(t)}" title="o^{(t)}" /></a>
并得到任意t时刻的最终输出
<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{y}^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{y}^{(t)}" title="\widehat{y}^{(t)}" /></a>


　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=o^{(t)}=Vh^{(t)}&plus;c" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o^{(t)}=Vh^{(t)}&plus;c" title="o^{(t)}=Vh^{(t)}+c" /></a>　　　　（公式2）

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{y}^{(t)}=\sigma&space;(o^{(t)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{y}^{(t)}=\sigma&space;(o^{(t)})" title="\widehat{y}^{(t)}=\sigma (o^{(t)})" /></a>　　　　（公式3）

其中，<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma" title="\sigma" /></a>
为激活函数，一般为softmax，c为线性偏置，相当于输出层阈值。

最后通过损失函数比较预测输出
<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{y}^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{y}^{(t)}" title="\widehat{y}^{(t)}" /></a>
与实际输出
<a href="https://www.codecogs.com/eqnedit.php?latex=y^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y^{(t)}" title="y^{(t)}" /></a>
，如果不满足要求，则要进行反向传播，更新参数U、V、W、b、c。

## 4、反向传播

反向传播时，用到的是梯度下降法。我们首先需要对损失进行重新理解。在BP神经网络中，每次训练完成都用到一次损失函数计算预测输出与实际输出的误差。RNN的反向传播同样如此，但是每一时刻的数据输入都会调用一次损失函数，因此最终的损失L定义为：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=L=\sum_{t=1}^{\tau&space;}L^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L=\sum_{t=1}^{\tau&space;}L^{(t)}" title="L=\sum_{t=1}^{\tau }L^{(t)}" /></a>　　　　（公式4）

有了损失函数后，我们开始从易到难计算梯度。为了简化描述，损失函数我们选用交叉熵损失函数

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=L^{(t)}=-y^{(t)}\cdot&space;log(\widehat{y}^{(t)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L^{(t)}=-y^{(t)}\cdot&space;log(\widehat{y}^{(t)})" title="L^{(t)}=-y^{(t)}\cdot log(\widehat{y}^{(t)})" /></a>　　　　（公式5）

输出层激活函数选用softmax函数

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=S_{i}=\tfrac{e^{i}}{\sum_{j=1}^{n}e^{j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?S_{i}=\tfrac{e^{i}}{\sum_{j=1}^{n}e^{j}}" title="S_{i}=\tfrac{e^{i}}{\sum_{j=1}^{n}e^{j}}" /></a>

隐层激活函数选用<a href="https://www.codecogs.com/eqnedit.php?latex=tanh" target="_blank"><img src="https://latex.codecogs.com/gif.latex?tanh" title="tanh" /></a>函数

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=tanh(z)=\tfrac{e^{z}-e^{-z}}{e^{z}&plus;e^{-z}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?tanh(z)=\tfrac{e^{z}-e^{-z}}{e^{z}&plus;e^{-z}}" title="tanh(z)=\tfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}}" /></a>

开始计算梯度：

1）V、c的梯度

根据（公式4）（公式5）（公式3）（公式2）以及交叉熵函数和softmax函数的求导法则

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;c}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;c}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;\widehat{y}^{(t)}}\cdot&space;\frac{\partial&space;\widehat{y}^{(t)}}{\partial&space;o^{(t)}}\cdot&space;\frac{\partial&space;o^{(t)}}{\partial&space;c}=\sum_{t=1}^{\tau&space;}\widehat{y}^{(t)}-y^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;c}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;c}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;\widehat{y}^{(t)}}\cdot&space;\frac{\partial&space;\widehat{y}^{(t)}}{\partial&space;o^{(t)}}\cdot&space;\frac{\partial&space;o^{(t)}}{\partial&space;c}=\sum_{t=1}^{\tau&space;}\widehat{y}^{(t)}-y^{(t)}" title="\frac{\partial L}{\partial c}=\sum_{t=1}^{\tau }\frac{\partial L^{(t)}}{\partial c}=\sum_{t=1}^{\tau }\frac{\partial L^{(t)}}{\partial \widehat{y}^{(t)}}\cdot \frac{\partial \widehat{y}^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial c}=\sum_{t=1}^{\tau }\widehat{y}^{(t)}-y^{(t)}" /></a>（公式6）

同理，

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;V}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;V}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;\widehat{y}^{(t)}}\cdot&space;\frac{\partial&space;\widehat{y}^{(t)}}{\partial&space;o^{(t)}}\cdot&space;\frac{\partial&space;o^{(t)}}{\partial&space;V}=\sum_{t=1}^{\tau&space;}(\widehat{y}^{(t)}-y^{(t)})(h(t))^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;V}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;V}=\sum_{t=1}^{\tau&space;}\frac{\partial&space;L^{(t)}}{\partial&space;\widehat{y}^{(t)}}\cdot&space;\frac{\partial&space;\widehat{y}^{(t)}}{\partial&space;o^{(t)}}\cdot&space;\frac{\partial&space;o^{(t)}}{\partial&space;V}=\sum_{t=1}^{\tau&space;}(\widehat{y}^{(t)}-y^{(t)})(h(t))^{T}" title="\frac{\partial L}{\partial V}=\sum_{t=1}^{\tau }\frac{\partial L^{(t)}}{\partial V}=\sum_{t=1}^{\tau }\frac{\partial L^{(t)}}{\partial \widehat{y}^{(t)}}\cdot \frac{\partial \widehat{y}^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}=\sum_{t=1}^{\tau }(\widehat{y}^{(t)}-y^{(t)})(h(t))^{T}" /></a>（公式7）

2）W、U、b的梯度

由于这三个变量均在（公式1）中，而且（公式1）涉及到了历史数据，求导复杂，所以计算梯度很麻烦。从RNN模型中我们可以看到，某一时刻t的梯度损失是由当前位置的损失和后一时刻也就是t+1时的损失共同决定（反向传播）。对此，我们可以先计算损失函数在某一时刻对于
<a href="https://www.codecogs.com/eqnedit.php?latex=h(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(t)" title="h(t)" /></a>
的梯度（全导数公式）：

<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;(t)=\frac{\partial&space;L(t)}{\partial&space;o^{(t)}}\frac{\partial&space;o^{(t)}}{\partial&space;h(t)}&plus;\frac{\partial&space;L(t)}{\partial&space;h(t&plus;1)}\frac{\partial&space;h(t&plus;1)}{\partial&space;h(t)}=(\widehat{y}^{(t)}-y^{(t)})V&plus;\delta&space;(t&plus;1)diag(1-(h^{t&plus;1})^{2})W" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;(t)=\frac{\partial&space;L(t)}{\partial&space;o^{(t)}}\frac{\partial&space;o^{(t)}}{\partial&space;h(t)}&plus;\frac{\partial&space;L(t)}{\partial&space;h(t&plus;1)}\frac{\partial&space;h(t&plus;1)}{\partial&space;h(t)}=(\widehat{y}^{(t)}-y^{(t)})V&plus;\delta&space;(t&plus;1)diag(1-(h^{t&plus;1})^{2})W" title="\delta (t)=\frac{\partial L(t)}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h(t)}+\frac{\partial L(t)}{\partial h(t+1)}\frac{\partial h(t+1)}{\partial h(t)}=(\widehat{y}^{(t)}-y^{(t)})V+\delta (t+1)diag(1-(h^{t+1})^{2})W" /></a>

对于最后一个时刻
<a href="https://www.codecogs.com/eqnedit.php?latex=\tau" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\tau" title="\tau" /></a>
后面没有其他的时刻了，因此：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;(\tau&space;)=(\widehat{y}^{(\tau&space;)}-y^{(\tau&space;)})V" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;(\tau&space;)=(\widehat{y}^{(\tau&space;)}-y^{(\tau&space;)})V" title="\delta (\tau )=(\widehat{y}^{(\tau )}-y^{(\tau )})V" /></a>

现在有了
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;(t)" title="\delta (t)" /></a>
我们可以很方便的计算W、U、b的梯度（需要
<a href="https://www.codecogs.com/eqnedit.php?latex=tanh" target="_blank"><img src="https://latex.codecogs.com/gif.latex?tanh" title="tanh" /></a>
函数的求导公式）：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;W}=\sum_{t=1}^{\tau&space;}\delta&space;(t)diag(1-(h^{t&plus;1})^{2})(h^{t-1})^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;W}=\sum_{t=1}^{\tau&space;}\delta&space;(t)diag(1-(h^{t&plus;1})^{2})(h^{t-1})^{T}" title="\frac{\partial L}{\partial W}=\sum_{t=1}^{\tau }\delta (t)diag(1-(h^{t+1})^{2})(h^{t-1})^{T}" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;U}=\sum_{t=1}^{\tau&space;}\delta&space;(t)diag(1-(h^{t&plus;1})^{2})(x^{(t)})^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;U}=\sum_{t=1}^{\tau&space;}\delta&space;(t)diag(1-(h^{t&plus;1})^{2})(x^{(t)})^{T}" title="\frac{\partial L}{\partial U}=\sum_{t=1}^{\tau }\delta (t)diag(1-(h^{t+1})^{2})(x^{(t)})^{T}" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;b}=\sum_{t=1}^{\tau&space;}\delta&space;(t)diag(1-(h^{t&plus;1})^{2})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;b}=\sum_{t=1}^{\tau&space;}\delta&space;(t)diag(1-(h^{t&plus;1})^{2})" title="\frac{\partial L}{\partial b}=\sum_{t=1}^{\tau }\delta (t)diag(1-(h^{t+1})^{2})" /></a>

至此所有的梯度变化就求完了，用当前的参数加上这个变化量得到新参数，再次进行前向传播，直到满足损失函数要求为止。

## 参考文献

https://www.cnblogs.com/pinard/p/6509630.html

https://blog.csdn.net/zhaojc1995/article/details/80572098




注：转载请注明，来自https://neuzhaoxin.github.io