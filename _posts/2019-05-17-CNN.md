---
layout: post
title:  "卷积神经网络（原理）"
date:   2019-05-17 15:50:36
categories: 机器学习
tags: 神经网络 CNN
---

* content
{:toc}

主要介绍一种适用于图像处理的神经网络：卷积神经网络(Convolutional Neural Networks ，以下简称CNN)，它广泛用于处理图像这样的有多个通道的输入数据，可以有效减少参数的个数。举个例子：一个28 * 28 * 1的图像，用普通神经网络的话，输入层就需要28 * 28=784个神经元，不管隐层和输出层的个数有多少，在784这个基数下都会变得很大。CNN则用其独特的方式解决这个问题。






## 1、卷积神经网络的层级结构

我们之前看过的BPNN有输入层、隐层、输出层三个层级，相应的，卷积神经网络（CNN）也有其独特的层级结构，分别是：输入层（Input）、卷积计算层（CONV）、激励层（RelU）、池化层（Pooling）、全连接层（FC）。

### 1.1输入层（Input）

该层要做的处理主要是对原始图像数据进行预处理，其中包括：

1）去均值：把输入数据各个维度都中心化为0，其目的就是把样本的中心拉回到坐标系原点上。

2）归一化：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。

3）PCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化

### 1.2 卷积计算层（CONV）

这一层是卷积神经网络最重要的一个层次，也是“卷积神经网络”的名字来源。

在这个卷积层，有两个关键操作：

1）局部关联。每个神经元看做一个滤波器(filter)

2）窗口(receptive field)滑动， filter对局部数据计算

先介绍卷积层遇到的几个名词：

1）深度/depth：有多少个神经元，深度就是多少
2）步长/stride：窗口一次滑动的长度
3）填充值/zero-padding：避免滑动窗口运行时，有剩余元素没经过。如图：

有一张5 * 5的图片，滑动窗取2 * 2，如果步长为2的话，向右滑动，刚好剩一个像素。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/55滑动窗.jpg)

这时候我们在矩阵外加一层填充层，变成6 * 6的矩阵，这样刚好所有像素都处理了。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/66滑动窗.jpg)

我们知道了窗口滑动，那窗口到了它所在的位置，应该如何进行卷积计算呢？

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/卷积1.jpg)

如上图所示，蓝色矩阵是输入图像，粉色矩阵是卷积层神经元，绿色矩阵是卷积后的输出。按照我们刚刚介绍的知识，卷积层的深度为2，因为有W0、W1两个神经元，滑动窗选取的是3 * 3的，输入图像最外面一层灰色部分为填充值。具体的卷积过程是这样的，输入层滑动窗内的数据与神经元对应权值数据一一相乘并相加，最后再加上线性偏置得到卷积输出。上图中-3的计算过程是这样的：

<a href="https://www.codecogs.com/eqnedit.php?latex=x[:,:,0]\circ&space;W0[:,:,0]&plus;x[:,:,1]\circ&space;W0[:,:,1]&plus;x[:,:,2]\circ&space;W0[:,:,2]&plus;b=0&plus;(-4)&plus;0&plus;1=-3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x[:,:,0]\circ&space;W0[:,:,0]&plus;x[:,:,1]\circ&space;W0[:,:,1]&plus;x[:,:,2]\circ&space;W0[:,:,2]&plus;b=0&plus;(-4)&plus;0&plus;1=-3" title="x[:,:,0]\circ W0[:,:,0]+x[:,:,1]\circ W0[:,:,1]+x[:,:,2]\circ W0[:,:,2]+b=0+(-4)+0+1=-3" /></a>

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/卷积2.jpg)

如上图，经历了2个步长之后，继续计算第二个输出：

<a href="https://www.codecogs.com/eqnedit.php?latex=x[:,:,0]\circ&space;W0[:,:,0]&plus;x[:,:,1]\circ&space;W0[:,:,1]&plus;x[:,:,2]\circ&space;W0[:,:,2]&plus;b=2&plus;(-3)&plus;(-3)&plus;1=-3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x[:,:,0]\circ&space;W0[:,:,0]&plus;x[:,:,1]\circ&space;W0[:,:,1]&plus;x[:,:,2]\circ&space;W0[:,:,2]&plus;b=2&plus;(-3)&plus;(-3)&plus;1=-3" title="x[:,:,0]\circ W0[:,:,0]+x[:,:,1]\circ W0[:,:,1]+x[:,:,2]\circ W0[:,:,2]+b=2+(-3)+(-3)+1=-3" /></a>

这样逐步进行，就完成了卷积计算。

### 1.3 激励层（RelU）

把卷积层输出结果做非线性映射，相当于普通神经网络中的激励函数。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/激励层.jpg)

CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/RelU.jpg)

### 1.4 池化层（Pooling）

池化层一般在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。

池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。

这里就说一下Max pooling，其实思想非常简单。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/maxpooling.jpg)

如上图，对于每个2 * 2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2 * 2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。

### 1.5 全连接层（FC）

全连接层跟传统的神经网络神经元的连接方式是一样的，它的作用是对经过多次卷积层和多次池化层所得出来的高级特征进行全连接（全连接就是常规神经网络的性质），也就是将池化后的数据“拍平”，放入全连接层，算出最后的预测值或者说采用softmax对其进行分类。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/全连接层.jpg)

## 2、前向传播

前面我们只是介绍了几个CNN的层级，并没有说他们的具体顺序，其实这几个层级的顺序可以通过一个公式来表达：

<a href="https://www.codecogs.com/eqnedit.php?latex=INPUT\rightarrow&space;[[CONV\rightarrow&space;RELU]*N&space;\rightarrow&space;POOL?]*M&space;\rightarrow&space;[FC&space;\rightarrow&space;RELU]*K&space;\rightarrow&space;FC" target="_blank"><img src="https://latex.codecogs.com/gif.latex?INPUT\rightarrow&space;[[CONV\rightarrow&space;RELU]*N&space;\rightarrow&space;POOL?]*M&space;\rightarrow&space;[FC&space;\rightarrow&space;RELU]*K&space;\rightarrow&space;FC" title="INPUT\rightarrow [[CONV\rightarrow RELU]*N \rightarrow POOL?]*M \rightarrow [FC \rightarrow RELU]*K \rightarrow FC" /></a>

其中，N代表卷积并激励的次数，一般
<a href="https://www.codecogs.com/eqnedit.php?latex=0\leqslant&space;N\leqslant&space;3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0\leqslant&space;N\leqslant&space;3" title="0\leqslant N\leqslant 3" /></a>；

？表示池化进行0次或一次；

M代表卷积、激励并池化的次数，一般
<a href="https://www.codecogs.com/eqnedit.php?latex=0\leqslant&space;M" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0\leqslant&space;M" title="0\leqslant M" /></a>

K表示全连接进行次数，一般
<a href="https://www.codecogs.com/eqnedit.php?latex=0\leqslant&space;K\leqslant&space;3" target="_blank"><img src="https://latex.codecogs.com/gif.latex?0\leqslant&space;K\leqslant&space;3" title="0\leqslant K\leqslant 3" /></a>

一个最简单的CNN层级结构如下图所示：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/CNN层级结构.jpg)

下面，进行前向传播。

### 2.1 输入层-->卷积层

如下图所示，假定输入是如图所示4 * 4的图像，经过两个2 * 2的卷积核以步长1进行卷积运算，最终变成3 * 3的输出。 

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/输入到卷积.jpg)

以卷积核filter1为例：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/filter1.jpg)

计算第一个卷积层神经元<a href="https://www.codecogs.com/eqnedit.php?latex=o_{11}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o_{11}" title="o_{11}" /></a>的输入:

<a href="https://www.codecogs.com/eqnedit.php?latex=net_{o_{11}}=conv(input,filter)=i_{11}×h_{11}&plus;i_{12}×h_{12}&plus;i_{21}×h_{21}&plus;i_{22}×h_{22}=1\times&space;1&plus;0\times&space;(-1)&plus;1\times&space;1&plus;1\times&space;(-1)=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?net_{o_{11}}=conv(input,filter)=i_{11}×h_{11}&plus;i_{12}×h_{12}&plus;i_{21}×h_{21}&plus;i_{22}×h_{22}=1\times&space;1&plus;0\times&space;(-1)&plus;1\times&space;1&plus;1\times&space;(-1)=1" title="net_{o_{11}}=conv(input,filter)=i_{11}×h_{11}+i_{12}×h_{12}+i_{21}×h_{21}+i_{22}×h_{22}=1\times 1+0\times (-1)+1\times 1+1\times (-1)=1" /></a>

再利用RelU函数激活：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=out_{0_{11}}=max(0,net_{o_{11}})=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?out_{0_{11}}=max(0,net_{o_{11}})=1" title="out_{0_{11}}=max(0,net_{o_{11}})=1" /></a>

其他的计算方式与上述相同。

### 2.2 卷积层-->池化层

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/卷积到池化.jpg)

利用Max pooling的方式对卷积层输出进行池化，由于池化层无激活函数，因此：

　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=out_{m_{11}}=net_{m_{11}}=max(o_{11},o_{12},o_{21},o_{22})=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?out_{m_{11}}=net_{m_{11}}=max(o_{11},o_{12},o_{21},o_{22})=1" title="out_{m_{11}}=net_{m_{11}}=max(o_{11},o_{12},o_{21},o_{22})=1" /></a>

其他的计算方式与上述相同。

### 2.3 池化层-->全连接层

把池化层输出的所有元素“拍平”，然后输出到全连接层。全连接层与普通神经网络一样。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/池化到全连接.jpg)

因此：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=y=\sigma&space;(\sum_{i=1}^{n}\theta&space;_{i}^{T}x_{i}&plus;b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=\sigma&space;(\sum_{i=1}^{n}\theta&space;_{i}^{T}x_{i}&plus;b)" title="y=\sigma (\sum_{i=1}^{n}\theta _{i}^{T}x_{i}+b)" /></a>

## 3、反向传播

### 3.1 全连接层

与普通神经网络完全相同：

1）通过前向传播计算每一层的输入值
<a href="https://www.codecogs.com/eqnedit.php?latex=net_{i,j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?net_{i,j}" title="net_{i,j}" /></a>

2）反向传播计算每个神经元的误差项（梯度）
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{i,j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{i,j}" title="\delta _{i,j}" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{i,j}=\frac{\partial&space;E}{\partial&space;net_{i,j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{i,j}=\frac{\partial&space;E}{\partial&space;net_{i,j}}" title="\delta _{i,j}=\frac{\partial E}{\partial net_{i,j}}" /></a>

其中E为损失函数计算得到的总体误差，可以用平方差，交叉熵等表示。

3）计算每个神经元权重
<a href="https://www.codecogs.com/eqnedit.php?latex=w&space;_{i,j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w&space;_{i,j}" title="w _{i,j}" /></a>
的梯度

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\eta&space;_{i,j}=\frac{\partial&space;E}{\partial&space;net_{i,j}}\cdot&space;\frac{\partial&space;net_{i,j}}{\partial&space;w&space;_{i,j}}=\delta&space;_{i,j}\cdot&space;out_{i,j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\eta&space;_{i,j}=\frac{\partial&space;E}{\partial&space;net_{i,j}}\cdot&space;\frac{\partial&space;net_{i,j}}{\partial&space;w&space;_{i,j}}=\delta&space;_{i,j}\cdot&space;out_{i,j}" title="\eta _{i,j}=\frac{\partial E}{\partial net_{i,j}}\cdot \frac{\partial net_{i,j}}{\partial w _{i,j}}=\delta _{i,j}\cdot out_{i,j}" /></a>

4）更新权重 
<a href="https://www.codecogs.com/eqnedit.php?latex=w&space;_{i,j}=w&space;_{i,j}-\lambda&space;\cdot&space;\eta&space;_{i,j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w&space;_{i,j}=w&space;_{i,j}-\lambda&space;\cdot&space;\eta&space;_{i,j}" title="w _{i,j}=w _{i,j}-\lambda \cdot \eta _{i,j}" /></a>
(其中<a href="https://www.codecogs.com/eqnedit.php?latex=\lambda" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" /></a>为学习率)

### 3.2 池化层

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/卷积到池化.jpg)

在这里，我们用
<a href="https://www.codecogs.com/eqnedit.php?latex=i_{11}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i_{11}" title="i_{11}" /></a>
表示前一层，用
<a href="https://www.codecogs.com/eqnedit.php?latex=o_{11}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o_{11}" title="o_{11}" /></a>
表示后一层，
<a href="https://www.codecogs.com/eqnedit.php?latex=net_{i_{11}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?net_{i_{11}}" title="net_{i_{11}}" /></a>
为前一个神经元的输入，
<a href="https://www.codecogs.com/eqnedit.php?latex=out_{i_{11}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?out_{i_{11}}" title="out_{i_{11}}" /></a>
为前一个神经元的输出，
<a href="https://www.codecogs.com/eqnedit.php?latex=net_{o_{11}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?net_{o_{11}}" title="net_{o_{11}}" /></a>
为后一个神经元的输入，
<a href="https://www.codecogs.com/eqnedit.php?latex=out_{o_{11}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?out_{o_{11}}" title="out_{o_{11}}" /></a>
为后一个神经元的输出。

根据池化层前向传播过程：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=net_{m_{11}}=max(out_{o_{11}},out_{o_{12}},out_{o_{21}},out_{o_{22}})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?net_{m_{11}}=max(out_{o_{11}},out_{o_{12}},out_{o_{21}},out_{o_{22}})" title="net_{m_{11}}=max(out_{o_{11}},out_{o_{12}},out_{o_{21}},out_{o_{22}})" /></a>

假定该过程中最大值为
<a href="https://www.codecogs.com/eqnedit.php?latex=out_{o_{11}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?out_{o_{11}}" title="out_{o_{11}}" /></a>

将误差分别放入对应的数据最大的位置，由于没有激活函数，则：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{11}}}=1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{11}}}=1" title="\frac{\partial net_{m_{11}}}{\partial out_{o_{11}}}=1" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{12}}}=\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{21}}}=\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{22}}}=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{12}}}=\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{21}}}=\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{22}}}=0" title="\frac{\partial net_{m_{11}}}{\partial out_{o_{12}}}=\frac{\partial net_{m_{11}}}{\partial out_{o_{21}}}=\frac{\partial net_{m_{11}}}{\partial out_{o_{22}}}=0" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{11}^{l-1}=\frac{\partial&space;E}{\partial&space;out_{o_{11}}}=\frac{\partial&space;E}{\partial&space;net_{m_{11}}}\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{11}}}=\delta&space;_{11}^{l}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{11}^{l-1}=\frac{\partial&space;E}{\partial&space;out_{o_{11}}}=\frac{\partial&space;E}{\partial&space;net_{m_{11}}}\frac{\partial&space;net_{m_{11}}}{\partial&space;out_{o_{11}}}=\delta&space;_{11}^{l}" title="\delta _{11}^{l-1}=\frac{\partial E}{\partial out_{o_{11}}}=\frac{\partial E}{\partial net_{m_{11}}}\frac{\partial net_{m_{11}}}{\partial out_{o_{11}}}=\delta _{11}^{l}" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{12}^{l-1}=\delta&space;_{21}^{l-1}=\delta&space;_{22}^{l-1}=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{12}^{l-1}=\delta&space;_{21}^{l-1}=\delta&space;_{22}^{l-1}=0" title="\delta _{12}^{l-1}=\delta _{21}^{l-1}=\delta _{22}^{l-1}=0" /></a>

同理可以求出每个神经元的梯度并更新权重。

### 3.3 卷积层

由前向传播可知：

<a href="https://www.codecogs.com/eqnedit.php?latex=out_{i_{11}}=max(0,net_{i_{11}})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?out_{i_{11}}=max(0,net_{i_{11}})" title="out_{i_{11}}=max(0,net_{i_{11}})" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=net_{o_{11}}=i_{11}×h_{11}&plus;i_{12}×h_{12}&plus;i_{21}×h_{21}&plus;i_{22}×h_{22}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?net_{o_{11}}=i_{11}×h_{11}&plus;i_{12}×h_{12}&plus;i_{21}×h_{21}&plus;i_{22}×h_{22}" title="net_{o_{11}}=i_{11}×h_{11}+i_{12}×h_{12}+i_{21}×h_{21}+i_{22}×h_{22}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=out_{o_{11}}=max(0,net_{o_{11}})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?out_{o_{11}}=max(0,net_{o_{11}})" title="out_{o_{11}}=max(0,net_{o_{11}})" /></a>

首先计算卷积的上一层的第一个元素
<a href="https://www.codecogs.com/eqnedit.php?latex=i_{11}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i_{11}" title="i_{11}" /></a>的误差项
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{11}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{11}" title="\delta _{11}" /></a>：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{11}=\frac{\partial&space;E}{\partial&space;net_{i_{11}}}=\frac{\partial&space;E}{\partial&space;out_{i_{11}}}\frac{\partial&space;out_{i_{11}}}{\partial&space;net_{i_{11}}}=\frac{\partial&space;E}{\partial&space;i_{11}}\frac{\partial&space;i_{11}}{\partial&space;net_{i_{11}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{11}=\frac{\partial&space;E}{\partial&space;net_{i_{11}}}=\frac{\partial&space;E}{\partial&space;out_{i_{11}}}\frac{\partial&space;out_{i_{11}}}{\partial&space;net_{i_{11}}}=\frac{\partial&space;E}{\partial&space;i_{11}}\frac{\partial&space;i_{11}}{\partial&space;net_{i_{11}}}" title="\delta _{11}=\frac{\partial E}{\partial net_{i_{11}}}=\frac{\partial E}{\partial out_{i_{11}}}\frac{\partial out_{i_{11}}}{\partial net_{i_{11}}}=\frac{\partial E}{\partial i_{11}}\frac{\partial i_{11}}{\partial net_{i_{11}}}" /></a>

先计算
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;E}{\partial&space;i_{11}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;i_{11}}" title="\frac{\partial E}{\partial i_{11}}" /></a>

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/规律1.jpg)

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/规律2.jpg)

仔细观察，我们可以得到如下规律：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/CNN/规律3.jpg)

因此

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;E}{\partial&space;i_{i,j}}=\sum&space;_{m}\sum&space;_{n}h_{m,n}\delta&space;_{i&plus;m,j&plus;n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;i_{i,j}}=\sum&space;_{m}\sum&space;_{n}h_{m,n}\delta&space;_{i&plus;m,j&plus;n}" title="\frac{\partial E}{\partial i_{i,j}}=\sum _{m}\sum _{n}h_{m,n}\delta _{i+m,j+n}" /></a>

接下来计算
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;i_{11}}{\partial&space;net_{i_{11}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;i_{11}}{\partial&space;net_{i_{11}}}" title="\frac{\partial i_{11}}{\partial net_{i_{11}}}" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;i_{11}}{\partial&space;net_{i_{11}}}=f'(net_{i_{11}})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;i_{11}}{\partial&space;net_{i_{11}}}=f'(net_{i_{11}})" title="\frac{\partial i_{11}}{\partial net_{i_{11}}}=f'(net_{i_{11}})" /></a>

最终

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{11}=\sum&space;_{m}\sum&space;_{n}h_{m,n}\delta&space;_{i&plus;m,j&plus;n}\cdot&space;f'(net_{i_{11}})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{11}=\sum&space;_{m}\sum&space;_{n}h_{m,n}\delta&space;_{i&plus;m,j&plus;n}\cdot&space;f'(net_{i_{11}})" title="\delta _{11}=\sum _{m}\sum _{n}h_{m,n}\delta _{i+m,j+n}\cdot f'(net_{i_{11}})" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;E}{\partial&space;h_{i,j}}=\sum&space;_{m}\sum&space;_{n}\delta&space;_{m,n}out_{o_{i&plus;m,j&plus;n}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;h_{i,j}}=\sum&space;_{m}\sum&space;_{n}\delta&space;_{m,n}out_{o_{i&plus;m,j&plus;n}}" title="\frac{\partial E}{\partial h_{i,j}}=\sum _{m}\sum _{n}\delta _{m,n}out_{o_{i+m,j+n}}" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;E}{\partial&space;b}=\sum&space;_{m}\sum&space;_{n}\delta&space;_{i,j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E}{\partial&space;b}=\sum&space;_{m}\sum&space;_{n}\delta&space;_{i,j}" title="\frac{\partial E}{\partial b}=\sum _{m}\sum _{n}\delta _{i,j}" /></a>

至此所有的参数均可以更新了。

## 参考文献

https://www.cnblogs.com/charlotte77/p/7759802.html

https://www.cnblogs.com/charlotte77/p/7783261.html

https://blog.csdn.net/liangchunjiang/article/details/79030681

https://www.cnblogs.com/skyfsm/p/6790245.html

https://yq.aliyun.com/articles/637953

注：转载请注明，来自https://neuzhaoxin.github.io