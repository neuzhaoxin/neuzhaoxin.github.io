---
layout: post
title:  "长短时记忆神经网络（原理）"
date:   2019-05-16 8:47:26
categories: 机器学习
tags: 神经网络 RNN LSTM
---

* content
{:toc}

主要介绍一种循环神经网络(RNN)的改进模型：长短时记忆神经网络（Long Short-Term Memory，LSTM）。在一些复杂语言场景中，有用信息的间隔有大有小、长短不一，简单的RNN网络的性能会受到限制。而LSTM网络的设计就是为了解决该问题。





## 1、LSTM网络模型

在RNN模型中，后面的数据会受到前面数据的影响，每一层加入了一个隐藏状态
<a href="https://www.codecogs.com/eqnedit.php?latex=h(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(t)" title="h(t)" /></a>。

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/RNN/RNN模型.jpg)

如果我们不去看共有的输出模型，其余部分可以简化成下图：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/RNN模型简化.jpg)

但是对于这样一个模型，间隔越远数据的影响越不明显，但是我们无法保证，一段较长时间之前的数据对现在的数据影响比前一时刻对现在的数据影响要小。除此之外，RNN网络还存在梯度消失的问题。为此，提出了网络的变形——LSYM。其结构如下图所示：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/LSTM模型.jpg)

这样一个结构看上去比较复杂，因此我们可以拆解开进行分析，拆解过后主要分为以下几部分：细胞状态、遗忘门、输入门、细胞状态更新、输出门。接下来逐一进行介绍。

### 1.1 细胞状态

在LSTM模型中，除了前一时刻的隐藏状态<a href="https://www.codecogs.com/eqnedit.php?latex=h(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(t)" title="h(t)" /></a>
之外，还多了另一个隐藏状态，这个状态的目的就是保存较长一段时间前的数据，我们一般将这个状态称为细胞状态（Cell State），记为
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}" title="C^{(t)}" /></a>。过程如下图所示：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/细胞状态.jpg)

### 1.2 遗忘门

遗忘门（forget gate）的目的是控制是否遗忘，即在LSTM中以一定的概率控制是否遗忘上一层细胞状态。其结构如下图所示：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/遗忘门.jpg)

图中遗忘门的输入为上一时间序列隐藏状态
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t-1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t-1)}" title="h^{(t-1)}" /></a>
和本时间序列数据
<a href="https://www.codecogs.com/eqnedit.php?latex=x^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x^{(t)}" title="x^{(t)}" /></a>
，两项输入通过激活函数（一般为sigmoid函数）得到输出
<a href="https://www.codecogs.com/eqnedit.php?latex=f^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f^{(t)}" title="f^{(t)}" /></a>
，这个输出代表了遗忘上一层隐藏状态的概率，数学表达式为：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=f^{(t)}=\sigma&space;(W_{f}h^{(t-1)}&plus;U_{f}x^{(t)}&plus;b_{f})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f^{(t)}=\sigma&space;(W_{f}h^{(t-1)}&plus;U_{f}x^{(t)}&plus;b_{f})" title="f^{(t)}=\sigma (W_{f}h^{(t-1)}+U_{f}x^{(t)}+b_{f})" /></a>

其中，
<a href="https://www.codecogs.com/eqnedit.php?latex=W_{f}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{f}" title="W_{f}" /></a>
和
<a href="https://www.codecogs.com/eqnedit.php?latex=U_{f}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?U_{f}" title="U_{f}" /></a>
为线性权值，
<a href="https://www.codecogs.com/eqnedit.php?latex=b_{f}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b_{f}" title="b_{f}" /></a>
为阈值，
<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma" title="\sigma" /></a>
为激活函数。

### 1.3 输入门

输入门（input gate）负责处理当前时间序列的输入，其结构如下：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/输入门.jpg)

从图中可以看出，输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为
<a href="https://www.codecogs.com/eqnedit.php?latex=i^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{(t)}" title="i^{(t)}" /></a>
,第二部分使用了tanh激活函数，输出为
<a href="https://www.codecogs.com/eqnedit.php?latex=a^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(t)}" title="a^{(t)}" /></a>
, 两者的结果后面会相乘再去更新细胞状态。用数学表达式即为：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=i^{(t)}=\sigma&space;(W_{i}h^{(t-1)}&plus;U_{i}x^{(t)}&plus;b_{i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{(t)}=\sigma&space;(W_{i}h^{(t-1)}&plus;U_{i}x^{(t)}&plus;b_{i})" title="i^{(t)}=\sigma (W_{i}h^{(t-1)}+U_{i}x^{(t)}+b_{i})" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=a^{(t)}=tanh(W_{a}h^{(t-1)}&plus;U_{a}x^{(t)}&plus;b_{a})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(t)}=tanh(W_{a}h^{(t-1)}&plus;U_{a}x^{(t)}&plus;b_{a})" title="a^{(t)}=tanh(W_{a}h^{(t-1)}+U_{a}x^{(t)}+b_{a})" /></a>

其中，
<a href="https://www.codecogs.com/eqnedit.php?latex=W_{i},U_{i},W_{a},U_{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{i},U_{i},W_{a},U_{a}" title="W_{i},U_{i},W_{a},U_{a}" /></a>
为权值，
<a href="https://www.codecogs.com/eqnedit.php?latex=b_{i},b_{a}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b_{i},b_{a}" title="b_{i},b_{a}" /></a>
为阈值。

### 1.4 细胞状态更新

在研究LSTM输出门之前，我们要先看看LSTM之细胞状态。前面的遗忘门和输入门的结果都会作用于细胞状态
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}" title="C^{(t)}" /></a>。
我们来看看从细胞状态
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t-1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t-1)}" title="C^{(t-1)}" /></a>
如何得到
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}" title="C^{(t)}" /></a>。
如下图所示：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/细胞状态更新.jpg)

细胞状态
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}" title="C^{(t)}" /></a>
由两部分组成，第一部分是
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t-1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t-1)}" title="C^{(t-1)}" /></a>
和遗忘门输出
<a href="https://www.codecogs.com/eqnedit.php?latex=f^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f^{(t)}" title="f^{(t)}" /></a>
的乘积，第二部分是输入门的
<a href="https://www.codecogs.com/eqnedit.php?latex=i^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{(t)}" title="i^{(t)}" /></a>
与输入门的
<a href="https://www.codecogs.com/eqnedit.php?latex=a^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(t)}" title="a^{(t)}" /></a>
的乘积，即：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}=C^{(t-1)}\odot&space;f^{(t)}&plus;i^{(t)}\odot&space;a^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}=C^{(t-1)}\odot&space;f^{(t)}&plus;i^{(t)}\odot&space;a^{(t)}" title="C^{(t)}=C^{(t-1)}\odot f^{(t)}+i^{(t)}\odot a^{(t)}" /></a>

其中，
<a href="https://www.codecogs.com/eqnedit.php?latex=\odot" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\odot" title="\odot" /></a>
为Hadamard积，也就是对应项相乘。

### 1.5 输出门

输出门结构如下：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/输出门.jpg)

从图中可以看出，隐藏状态
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t)}" title="h^{(t)}" /></a>
的更新由两部分组成，第一部分是
<a href="https://www.codecogs.com/eqnedit.php?latex=o^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o^{(t)}" title="o^{(t)}" /></a>
, 它由上一序列的隐藏状态
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t-1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t-1)}" title="h^{(t-1)}" /></a>
和本序列数据
<a href="https://www.codecogs.com/eqnedit.php?latex=x^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x^{(t)}" title="x^{(t)}" /></a>
，以及激活函数sigmoid得到，第二部分由隐藏状态
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}" title="C^{(t)}" /></a>
和tanh激活函数组成, 即：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=o^{(t)}=\sigma&space;(W_{o}h^{(t-1)}&plus;U_{o}x^{(t)}&plus;b_{o})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o^{(t)}=\sigma&space;(W_{o}h^{(t-1)}&plus;U_{o}x^{(t)}&plus;b_{o})" title="o^{(t)}=\sigma (W_{o}h^{(t-1)}+U_{o}x^{(t)}+b_{o})" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t)}=o^{(t)}\odot&space;tanh(C^{(t)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t)}=o^{(t)}\odot&space;tanh(C^{(t)})" title="h^{(t)}=o^{(t)}\odot tanh(C^{(t)})" /></a>

其中，
<a href="https://www.codecogs.com/eqnedit.php?latex=W_{o},U_{o},b_{o}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{o},U_{o},b_{o}" title="W_{o},U_{o},b_{o}" /></a>
分别为权值和阈值。

## 2、前向传播

按照上述结构，在每个门中依次进行数据处理，并进行最后的输出，达到前向传播的目的。具体过程如下：

1）更新遗忘门输出：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=f^{(t)}=\sigma&space;(W_{f}h^{(t-1)}&plus;U_{f}x^{(t)}&plus;b_{f})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f^{(t)}=\sigma&space;(W_{f}h^{(t-1)}&plus;U_{f}x^{(t)}&plus;b_{f})" title="f^{(t)}=\sigma (W_{f}h^{(t-1)}+U_{f}x^{(t)}+b_{f})" /></a>

2）更新输入门两个部分的输出：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=i^{(t)}=\sigma&space;(W_{i}h^{(t-1)}&plus;U_{i}x^{(t)}&plus;b_{i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?i^{(t)}=\sigma&space;(W_{i}h^{(t-1)}&plus;U_{i}x^{(t)}&plus;b_{i})" title="i^{(t)}=\sigma (W_{i}h^{(t-1)}+U_{i}x^{(t)}+b_{i})" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=a^{(t)}=tanh(W_{a}h^{(t-1)}&plus;U_{a}x^{(t)}&plus;b_{a})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(t)}=tanh(W_{a}h^{(t-1)}&plus;U_{a}x^{(t)}&plus;b_{a})" title="a^{(t)}=tanh(W_{a}h^{(t-1)}+U_{a}x^{(t)}+b_{a})" /></a>

3）更新细胞状态：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}=C^{(t-1)}\odot&space;f^{(t)}&plus;i^{(t)}\odot&space;a^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}=C^{(t-1)}\odot&space;f^{(t)}&plus;i^{(t)}\odot&space;a^{(t)}" title="C^{(t)}=C^{(t-1)}\odot f^{(t)}+i^{(t)}\odot a^{(t)}" /></a>

4）更新输出门输出：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=o^{(t)}=\sigma&space;(W_{o}h^{(t-1)}&plus;U_{o}x^{(t)}&plus;b_{o})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o^{(t)}=\sigma&space;(W_{o}h^{(t-1)}&plus;U_{o}x^{(t)}&plus;b_{o})" title="o^{(t)}=\sigma (W_{o}h^{(t-1)}+U_{o}x^{(t)}+b_{o})" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t)}=o^{(t)}\odot&space;tanh(C^{(t)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t)}=o^{(t)}\odot&space;tanh(C^{(t)})" title="h^{(t)}=o^{(t)}\odot tanh(C^{(t)})" /></a>

5）更新当前序列预测输出：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{y}^{(t)}=\sigma&space;(Vh^{(t)}&plus;c)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{y}^{(t)}=\sigma&space;(Vh^{(t)}&plus;c)" title="\widehat{y}^{(t)}=\sigma (Vh^{(t)}+c)" /></a>

## 3、反向传播

反向传播是通过梯度下降法迭代更新我们所有的参数，关键点在于计算所有参数基于损失函数的偏导数。在LSTM中由于有两个隐藏状态
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t)}" title="h^{(t)}" /></a>
和
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}" title="C^{(t)}" /></a>
，我们首先定义两个隐藏状态的梯度：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{h}^{(t)}=\frac{\partial&space;L}{\partial&space;h^{(t)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{h}^{(t)}=\frac{\partial&space;L}{\partial&space;h^{(t)}}" title="\delta _{h}^{(t)}=\frac{\partial L}{\partial h^{(t)}}" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{C}^{(t)}=\frac{\partial&space;L}{\partial&space;C^{(t)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{C}^{(t)}=\frac{\partial&space;L}{\partial&space;C^{(t)}}" title="\delta _{C}^{(t)}=\frac{\partial L}{\partial C^{(t)}}" /></a>

在这里L为损失函数。

为了便于推导，我们将损失函数L(t)分成两块，一块是时刻t位置的损失
<a href="https://www.codecogs.com/eqnedit.php?latex=l(t)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(t)" title="l(t)" /></a>
，另一块是时刻t之后损失L(t+1)，即：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=L(t)=\left\{\begin{matrix}&space;l(t)&plus;L(t&plus;1)&space;&&space;if(t<\tau&space;)\\&space;l(t)&space;&if(t=\tau)&space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(t)=\left\{\begin{matrix}&space;l(t)&plus;L(t&plus;1)&space;&&space;if(t<\tau&space;)\\&space;l(t)&space;&if(t=\tau)&space;\end{matrix}\right." title="L(t)=\left\{\begin{matrix} l(t)+L(t+1) & if(t<\tau )\\ l(t) &if(t=\tau) \end{matrix}\right." /></a>

其中，
<a href="https://www.codecogs.com/eqnedit.php?latex=\tau" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\tau" title="\tau" /></a>
为时间序列的最后时刻。

在
<a href="https://www.codecogs.com/eqnedit.php?latex=\tau" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\tau" title="\tau" /></a>
时刻的
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{h}^{(\tau&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{h}^{(\tau&space;)}" title="\delta _{h}^{(\tau )}" /></a>
和
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{C}^{(\tau&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{C}^{(\tau&space;)}" title="\delta _{C}^{(\tau )}" /></a>
为：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{h}^{(\tau&space;)}=\frac{\partial&space;L^{(\tau&space;)}}{\partial&space;O^{(\tau&space;)}}\frac{\partial&space;O^{(\tau&space;)}}{\partial&space;h^{(\tau&space;)}}=(\widehat{y}^{(\tau&space;)}-y^{(\tau&space;)})V" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{h}^{(\tau&space;)}=\frac{\partial&space;L^{(\tau&space;)}}{\partial&space;O^{(\tau&space;)}}\frac{\partial&space;O^{(\tau&space;)}}{\partial&space;h^{(\tau&space;)}}=(\widehat{y}^{(\tau&space;)}-y^{(\tau&space;)})V" title="\delta _{h}^{(\tau )}=\frac{\partial L^{(\tau )}}{\partial O^{(\tau )}}\frac{\partial O^{(\tau )}}{\partial h^{(\tau )}}=(\widehat{y}^{(\tau )}-y^{(\tau )})V" /></a>

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{C}^{(\tau&space;)}=\frac{\partial&space;L^{(\tau&space;)}}{\partial&space;h^{(\tau&space;)}}\frac{\partial&space;h^{(\tau&space;)}}{\partial&space;C^{(\tau&space;)}}=\delta&space;_{h}^{(\tau&space;)}\odot&space;o^{(\tau&space;)}\odot&space;(1-tanh^{2}(C^{(\tau&space;)}))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{C}^{(\tau&space;)}=\frac{\partial&space;L^{(\tau&space;)}}{\partial&space;h^{(\tau&space;)}}\frac{\partial&space;h^{(\tau&space;)}}{\partial&space;C^{(\tau&space;)}}=\delta&space;_{h}^{(\tau&space;)}\odot&space;o^{(\tau&space;)}\odot&space;(1-tanh^{2}(C^{(\tau&space;)}))" title="\delta _{C}^{(\tau )}=\frac{\partial L^{(\tau )}}{\partial h^{(\tau )}}\frac{\partial h^{(\tau )}}{\partial C^{(\tau )}}=\delta _{h}^{(\tau )}\odot o^{(\tau )}\odot (1-tanh^{2}(C^{(\tau )}))" /></a>

上式中的
<a href="https://www.codecogs.com/eqnedit.php?latex=O^{(\tau&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?O^{(\tau&space;)}" title="O^{(\tau )}" /></a>
指的不是输出门而是整体模型的未经损失函数处理的输出。

接着，我们由
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{C}^{(t&plus;1&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{C}^{(t&plus;1&space;)}" title="\delta _{C}^{(t+1 )}" /></a>
，
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{h}^{(t&plus;1&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{h}^{(t&plus;1&space;)}" title="\delta _{h}^{(t+1 )}" /></a>
反向推导
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{C}^{(t&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{C}^{(t&space;)}" title="\delta _{C}^{(t )}" /></a>
，
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{h}^{(t&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{h}^{(t&space;)}" title="\delta _{h}^{(t )}" /></a>
。

1）

<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{h}^{(t&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{h}^{(t&space;)}" title="\delta _{h}^{(t )}" /></a>
的梯度由本层t时刻的输出梯度误差和大于t时刻的误差两部分决定，即：

<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{h}^{(t&space;)}=\frac{\partial&space;L}{\partial&space;h^{(t)}}=\frac{\partial&space;l^{(t)}}{\partial&space;h^{(t)}}&plus;\frac{\partial&space;l^{(t&plus;1)}}{\partial&space;h^{(t&plus;1)}}\frac{\partial&space;h^{(t&plus;1)}}{\partial&space;h^{(t)}}=(\widehat{y}^{(t&space;)}-y^{(t)})V&plus;\delta&space;_{h}^{(t&plus;1&space;)}\frac{\partial&space;h^{(t&plus;1)}}{\partial&space;h^{(t)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{h}^{(t&space;)}=\frac{\partial&space;L}{\partial&space;h^{(t)}}=\frac{\partial&space;l^{(t)}}{\partial&space;h^{(t)}}&plus;\frac{\partial&space;l^{(t&plus;1)}}{\partial&space;h^{(t&plus;1)}}\frac{\partial&space;h^{(t&plus;1)}}{\partial&space;h^{(t)}}=(\widehat{y}^{(t&space;)}-y^{(t)})V&plus;\delta&space;_{h}^{(t&plus;1&space;)}\frac{\partial&space;h^{(t&plus;1)}}{\partial&space;h^{(t)}}" title="\delta _{h}^{(t )}=\frac{\partial L}{\partial h^{(t)}}=\frac{\partial l^{(t)}}{\partial h^{(t)}}+\frac{\partial l^{(t+1)}}{\partial h^{(t+1)}}\frac{\partial h^{(t+1)}}{\partial h^{(t)}}=(\widehat{y}^{(t )}-y^{(t)})V+\delta _{h}^{(t+1 )}\frac{\partial h^{(t+1)}}{\partial h^{(t)}}" /></a>

现在隐层h的梯度问题转化为求
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;h^{(t&plus;1)}}{\partial&space;h^{(t)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;h^{(t&plus;1)}}{\partial&space;h^{(t)}}" title="\frac{\partial h^{(t+1)}}{\partial h^{(t)}}" /></a>
。可以看到，在公式
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t)}=o^{(t)}\odot&space;tanh(C^{(t)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t)}=o^{(t)}\odot&space;tanh(C^{(t)})" title="h^{(t)}=o^{(t)}\odot tanh(C^{(t)})" /></a>
中，
<a href="https://www.codecogs.com/eqnedit.php?latex=o^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?o^{(t)}" title="o^{(t)}" /></a>
以及
<a href="https://www.codecogs.com/eqnedit.php?latex=C^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?C^{(t)}" title="C^{(t)}" /></a>
中的
<a href="https://www.codecogs.com/eqnedit.php?latex=f^{(t)},i^{(t)},a^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f^{(t)},i^{(t)},a^{(t)}" title="f^{(t)},i^{(t)},a^{(t)}" /></a>
均可以对下一时刻的h(t)求梯度。最终隐层h的梯度计算由四部分组成：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/隐层h计算.jpg)

其中，
<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;C=o^{(t&plus;1)}\odot&space;[1-tanh^{2}(C^{(t&plus;1)})]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;C=o^{(t&plus;1)}\odot&space;[1-tanh^{2}(C^{(t&plus;1)})]" title="\Delta C=o^{(t+1)}\odot [1-tanh^{2}(C^{(t+1)})]" /></a>

2）

<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{C}^{(t&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{C}^{(t&space;)}" title="\delta _{C}^{(t )}" /></a>的梯度由前一层
<a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{C}^{(t&plus;1&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{C}^{(t&plus;1&space;)}" title="\delta _{C}^{(t+1 )}" /></a>
的梯度误差和本层的从
<a href="https://www.codecogs.com/eqnedit.php?latex=h^{(t&space;)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h^{(t&space;)}" title="h^{(t )}" /></a>
传回来的梯度误差两部分组成，即：

![jpg](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/LSTM/隐层c计算.jpg)

有了两个隐层的梯度，计算参数就很容易了，以
<a href="https://www.codecogs.com/eqnedit.php?latex=W_{f}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{f}" title="W_{f}" /></a>
为例：

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;W_{f}}=\sum_{t=1}^{\tau&space;}[\delta&space;_{C}^{(t)}\odot&space;C^{(t&plus;1)}\odot&space;f^{(t)}\odot&space;(1-f^{(t)})](h^{(t-1)})^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;W_{f}}=\sum_{t=1}^{\tau&space;}[\delta&space;_{C}^{(t)}\odot&space;C^{(t&plus;1)}\odot&space;f^{(t)}\odot&space;(1-f^{(t)})](h^{(t-1)})^{T}" title="\frac{\partial L}{\partial W_{f}}=\sum_{t=1}^{\tau }[\delta _{C}^{(t)}\odot C^{(t+1)}\odot f^{(t)}\odot (1-f^{(t)})](h^{(t-1)})^{T}" /></a>

## 参考文献

https://www.cnblogs.com/pinard/p/6519110.html

https://www.cnblogs.com/ooon/p/5594438.html

https://www.jianshu.com/p/4b4701beba92




注：转载请注明，来自https://neuzhaoxin.github.io