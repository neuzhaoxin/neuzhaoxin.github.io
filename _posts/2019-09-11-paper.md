---
layout: post
title:  "paper"
date:   2019-09-11 8:57:24
categories: 论文
tags: 神经网络 注意力机制 LSTM 
---

* content
{:toc}

我看过的一些论文。



## 1、Electrode regulating system modeling in electrical smelting furnace using recurrent neural network with attention mechanism

### 1、背景

电炉（ESF）在冶炼过程中要保持冶炼电流稳定在合理范围内，这样有利于提高产品质量，降低能耗。

冶炼电流通过电极位置来调节。

因此建立了电极位置与冶炼电流之间的关系模型。

输入：三相电流的历史值、三相电极的控制信号、工作阶段
输出：下一时段的三相电流值

### 2、Encoder-Decoder

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/encoder-decoder-1.gif)

这里c表示将输入序列编码成一个固定长度的向量c，这个向量在解码的时候有两个作用

1、初始化decoder模型，进而可以得到第一个输出y1
2、指导每一个y的输出

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/原始.gif)

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/encoder-decoder-2.gif)

现在希望能通过输入得到多个c，每一个都能对应一个输出。

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/现在.gif)

### 3、attention mechanism

对应于神经网络回归问题就是，原来我用时间序列输入进行循环，得到最后一个时间段的隐层状态，进行输出。

现在我想把每一个时间段得到的隐层状态都利用起来，每一个隐层都加一个权值得到c。

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/对齐方式.jpg)

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/权值计算.jpg)

权值计算之前要计算编码端与解码端隐层的相关性，也就是decoder端生成位置i的结果时，有多少程度受encoder端的位置j的输入数据影响。最简单且最常用的对齐模型是dot product乘积矩阵，即把target端的输出隐状态ht与source端的输出隐状态进行矩阵乘。除此之外还有权值网络映射（General）和concat映射几种方式。

[这里](https://blog.csdn.net/LaoChengZier/article/details/87948445)有个实例。

## 2、Show, Attend and Tell: Neural Image Caption Generation with Visual Attention

### 1、乱七八糟

machine translation and object detection

standard backpropagation techniques

stochastically by maximizing a variational lower bound

We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO

be capable of capturing and expressing their relationships in a natural language.

it amounts to mimicking the remarkable human ability to compress huge amounts of salient visual infomation into descriptive language

Aided by advances in training neural networks and large classification datasets 

attention allows for salient features to dynamically come to the forefront as needed

Unfortunately, this has one potential drawback of losing information which could be useful for richer, more descriptive captions

We also show how one advantage of including attention is the ability to visualize what the model “sees”. 

Encouraged by recent advances in caption generation and inspired by recent success in employing attention in machine translation (Bahdanau et al., 2014) and object recognition (Ba et al., 2014; Mnih et al., 2014), we investigate models that can attend to salient part of an image while generating its caption

We introduce two attention-based image caption generators under a common framework : 1) a “soft” deterministic attention mechanism trainable by standard back-propagation methods and 2) a “hard” stochastic attention mechanism trainable by maximizing an approximate variational lower bound or equivalently by REINFORCE (Williams, 1992).

feature vector 

The first involved
generating caption templates which were filled in based on the results of object detections and attribute discovery

an intermediate “generalization” step

There has been a long line of previous work incorporating attention into neural networks for vision related tasks.

Some that share the same spirit as our work include

In particular however, our work directly extends the work of 

describe in detail in Section 4

We denote vectors with bolded font and matrices with capital letters.

we extract features from a lower convolutional layer unlike previous work which instead used a fully connected layer

## 3、Effective Long short-term Memory with Differential Evolution Algorithm for Electricity Price Prediction

### 1、乱七八糟

Experiments are conducted to verify the performance of the DE–LSTM model under the electricity prices in New South Wales, Germany/Austria, and France. Results indicate that the proposed DE–LSTM model outperforms existing forecasting models in terms of forecasting accuracies.

