---
layout: post
title:  "paper"
date:   2019-09-11 8:57:24
categories: 论文
tags: 神经网络 注意力机制 LSTM 
---

* content
{:toc}

我看过的一些论文。



## 1、Electrode regulating system modeling in electrical smelting furnace using recurrent neural network with attention mechanism

### 1、背景

电炉（ESF）在冶炼过程中要保持冶炼电流稳定在合理范围内，这样有利于提高产品质量，降低能耗。

冶炼电流通过电极位置来调节。

因此建立了电极位置与冶炼电流之间的关系模型。

输入：三相电流的历史值、三相电极的控制信号、工作阶段
输出：下一时段的三相电流值

### 2、Encoder-Decoder

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/encoder-decoder-1.gif)

这里c表示将输入序列编码成一个固定长度的向量c，这个向量在解码的时候有两个作用

1、初始化decoder模型，进而可以得到第一个输出y1
2、指导每一个y的输出

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/原始.gif)

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/encoder-decoder-2.gif)

现在希望能通过输入得到多个c，每一个都能对应一个输出。

![GIF](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/paper-Electrode-regulating-system-modeling/现在.gif)

### 3、attention mechanism

对应于神经网络回归问题就是，原来我用时间序列输入进行循环，得到最后一个时间段的隐层状态，进行输出。

现在我想把每一个时间段得到的隐层状态都利用起来，每一个隐层都加一个权值得到c。



## 2、Show, Attend and Tell: Neural Image Caption Generation with Visual Attention

### 1、乱七八糟

machine translation and object detection

standard backpropagation techniques

stochastically by maximizing a variational lower bound

We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO

be capable of capturing and expressing their relationships in a natural language.

it amounts to mimicking the remarkable human ability to compress huge amounts of salient visual infomation into descriptive language

Aided by advances in training neural networks and large classification datasets  