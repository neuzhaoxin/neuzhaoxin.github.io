---
layout: post
title:  "极限学习机(Extreme Learning Machine, ELM)"
date:   2019-09-30 10:02:28
categories: 算法
tags: 神经网络  
---

* content
{:toc}

极限学习机是神经网络一种新的想法，在保证学习精度的前提下提高速度。




## 1、引言

极限学习机神经网络结构上，就是一个前向传播的神经网络。其最大的创新点：

1）输入层和隐含层的连接权值、隐含层的阈值可以随机设定，且设定完后不用再调整。这和BP神经网络不一样，BP需要不断反向去调整权值和阈值。因此这里就能减少一半的运算量了。

2）隐含层和输出层之间的连接权值β不需要迭代调整，而是通过解方程组方式一次性确定。

研究表明，通过这样的规则，模型的泛化性能很好，速度提高了不少。

总而言之，ELM最大的特点就是对于传统的神经网络，尤其是单隐层前馈神经网络(SLFNs)，在保证学习精度的前提下比传统的学习算法速度更快。

## 2、原理

对于一个单隐层神经网络，如下图

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/ELM/单隐层.jpg)

假设有N个任意的样本
<a href="https://www.codecogs.com/eqnedit.php?latex=(X_{i},t_{i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(X_{i},t_{i})" title="(X_{i},t_{i})" /></a>，
其中
<a href="https://www.codecogs.com/eqnedit.php?latex=X_{i}=[x_{i1},x_{i2},...,x_{in}]^{T}\in&space;R^{m}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_{i}=[x_{i1},x_{i2},...,x_{in}]^{T}\in&space;R^{m}" title="X_{i}=[x_{i1},x_{i2},...,x_{in}]^{T}\in R^{m}" /></a>
<a href="https://www.codecogs.com/eqnedit.php?latex=t_{i}=[t_{i1},t_{i2},...,t_{im}]^{T}\in&space;R^{m}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?t_{i}=[t_{i1},t_{i2},...,t_{im}]^{T}\in&space;R^{m}" title="t_{i}=[t_{i1},t_{i2},...,t_{im}]^{T}\in R^{m}" /></a>。
对于一个有L个隐层节点的单隐层神经网络可以表示为

<a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^{L}\beta&space;_{i}g(W_{i}\cdot&space;X_{j}&plus;b_{i})=o_{j},j=1,...,N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{i=1}^{L}\beta&space;_{i}g(W_{i}\cdot&space;X_{j}&plus;b_{i})=o_{j},j=1,...,N" title="\sum_{i=1}^{L}\beta _{i}g(W_{i}\cdot X_{j}+b_{i})=o_{j},j=1,...,N" /></a>

其中，g(x)为激活函数，W为输入权重，<a href="https://www.codecogs.com/eqnedit.php?latex=\beta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta" /></a>为输出权重，b是第i个隐层单元的偏置。<a href="https://www.codecogs.com/eqnedit.php?latex=W_{i}\cdot&space;X_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{i}\cdot&space;X_{j}" title="W_{i}\cdot X_{j}" /></a>表示两者的内积。 

单层网络目标是使得输出的误差最小，即：

<a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{j=1}^{N}\left&space;\|&space;o_{j}-t_{j}&space;\right&space;\|=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{j=1}^{N}\left&space;\|&space;o_{j}-t_{j}&space;\right&space;\|=0" title="\sum_{j=1}^{N}\left \| o_{j}-t_{j} \right \|=0" /></a>

即存在βi，Wi和bi，使得

<a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^{L}\beta&space;_{i}g(W_{i}\cdot&space;X_{j}&plus;b_{i})=t_{i},j=1,...,N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{i=1}^{L}\beta&space;_{i}g(W_{i}\cdot&space;X_{j}&plus;b_{i})=t_{i},j=1,...,N" title="\sum_{i=1}^{L}\beta _{i}g(W_{i}\cdot X_{j}+b_{i})=t_{i},j=1,...,N" /></a>

可以矩阵表示为<a href="https://www.codecogs.com/eqnedit.php?latex=H\beta=&space;T" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H\beta=&space;T" title="H\beta= T" /></a>

其中，H是隐层节点的输出，<a href="https://www.codecogs.com/eqnedit.php?latex=\beta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta" /></a>为输出权重，T为期望输出。

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/ELM/成员.jpg)

为了能够训练单隐层神经网络，我们希望得到<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{W_{i}},\hat{b_{i}},\hat{\beta&space;_{i}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{W_{i}},\hat{b_{i}},\hat{\beta&space;_{i}}" title="\hat{W_{i}},\hat{b_{i}},\hat{\beta _{i}}" /></a>，使得

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/ELM/条件.jpg)

传统的一些基于梯度下降法的算法，可以用来求解这样的问题，但是基本的基于梯度的学习算法需要在迭代的过程中调整所有参数。而在ELM算法中, 一旦输入权重<a href="https://www.codecogs.com/eqnedit.php?latex=W_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{i}" title="W_{i}" /></a>和隐层的偏置<a href="https://www.codecogs.com/eqnedit.php?latex=b_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b_{i}" title="b_{i}" /></a>被随机确定，隐层的输出矩阵就被唯一确定。训练单隐层神经网络可以转化为求解一个线性系统<a href="https://www.codecogs.com/eqnedit.php?latex=H\beta=&space;T" target="_blank"><img src="https://latex.codecogs.com/gif.latex?H\beta=&space;T" title="H\beta= T" /></a>。

并且输出权重<a href="https://www.codecogs.com/eqnedit.php?latex=\beta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta" /></a>可以被确定。


## 参考文献

https://blog.csdn.net/lyxleft/article/details/82892383

https://blog.csdn.net/heli200482128/article/details/79149829