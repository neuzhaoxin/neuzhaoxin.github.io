---
layout: post
title:  "回声状态网络(Echo State Network, ESN)"
date:   2019-10-17 8:55:59
categories: 神经网络
tags: ESN RNN 
---

* content
{:toc}

回声状态网络的一些知识。




## 1、介绍

ESN是RNN的一种，也是由输入层，隐藏层，输出层组成，并且在隐藏层到隐藏层之间有一个连接，用来保留前面时刻留下的信息。不同于RNN，ESN的输入层到隐藏层、隐藏层到隐藏层的连接权值是随机初始化，并且固定不变。在训练的过程中，我们只需要去训练隐藏层到输出层的连接权值。这就变成了一个线性回归问题，所以ESN训练起来非常快。

## 2、网络结构

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/ESN/ESN.JPG)

ESN的神经网络如图所示，ESN通过随机地部署大规模系数链接的神经元构成网络隐层，一般称为"储备池"。ESN网络具有的特点如下：

(1)包含数目相对较多的神经元；

(2)神经元之间的连接关系随机产生；

(3)神经元之间的链接具有稀疏性；

网络主要三层结构构成：

*1.输入层(Input Layer)：*

输入向量u(n)其维度为：n×1

输入层→存储池的连接权重为：<a href="https://www.codecogs.com/eqnedit.php?latex=W_{in}^{m\times&space;n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{in}^{m\times&space;n}" title="W_{in}^{m\times n}" /></a>

**注意：该权重是不需要训练的，随机初始化完成即可**

*2.储存池(Reservoir)：*

存储池接受两个方向的输入一个来自于输入层 u(n)，另外一个来自存储池前一个状态的输出x(n−1),其中状态反馈权重<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{W}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{W}" title="\hat{W}" /></a>与<a href="https://www.codecogs.com/eqnedit.php?latex=W_{in}^{m\times&space;n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{in}^{m\times&space;n}" title="W_{in}^{m\times n}" /></a>相同均不需要训练，由随机初始状态决定，所以<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{W}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{W}" title="\hat{W}" /></a>为**大型稀疏矩阵**（在矩阵中，若数值为0的元素数目远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵），其中的非0元素表明了存储池中被激活的神经元：

<a href="https://www.codecogs.com/eqnedit.php?latex=x^{m\times&space;1}(n)=tanh(W_{in}^{m\times&space;n}u^{n\times&space;1}(n)&plus;\hat{W}x^{m\times&space;1}(n-1))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x^{m\times&space;1}(n)=tanh(W_{in}^{m\times&space;n}u^{n\times&space;1}(n)&plus;\hat{W}x^{m\times&space;1}(n-1))" title="x^{m\times 1}(n)=tanh(W_{in}^{m\times n}u^{n\times 1}(n)+\hat{W}x^{m\times 1}(n-1))" /></a>

*3.输出层(Readout)：*

存储池→输出层为线性连接关系，即满足：

<a href="https://www.codecogs.com/eqnedit.php?latex=y(n)=W_{out}^{l\times&space;m}x^{m\times&space;1}(n)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y(n)=W_{out}^{l\times&space;m}x^{m\times&space;1}(n)" title="y(n)=W_{out}^{l\times m}x^{m\times 1}(n)" /></a>

实际训练过程中需要训练线性连接的权重。

**核心思想：使用大规模随机稀疏网络(存储池)作为信息处理媒介，将输入信号从低维输入空间映射到高维度状态空间，在高维状态空间采用线性回归方法对网络的部分连接权重进行训练，而其他随机连接的权重在网络训练过程中保持不变。**

## 3、构造过程

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/ESN/构造流程.JPG)

首先进行**初始化**操作，在这里，我们先**确定储备池的大小**，即神经元的个数。与传统的MLP相同，节点数越多，拟合能力越强。由于ESN仅仅通过调整输出权值来线性拟合输出结果，所以一般ESN需要远大于一般神经网络的节点规模。 接下来是**随机生成连接矩阵**，这个矩阵表示了哪些神经元之间是有连接的，以及连接的方向和权值，实际上就是有向图的矩阵表示。接下来的**缩放矩阵**实际上就是归一化的操作，有些时候我们会直接使用一个缩放因子，使用该缩放因子乘以原始的随机生成的矩阵，相比于使用特征值来缩放更加快速，但是很大程度上也丧失了精度。为什么要进行这个操作呢？原因和我们在初始化一些神经网络的权值时是类似的，对于这些神经网络，我们通常会将权重初始化为0-1之间(或着-1至1)，有两个原因：(1)受激活函数的影响，比如使用sigmoid和tanh激活函数，在0与1之间区分度比较大，但是大于1之后，激活值变化不大；(2)我们对激活函数求导，可以看出在大于1时，图像比较平坦，其导数接近于0，由此在计算梯度时会导致梯度过小，无法顺利实现权重的更新。对于ESN我们不使用梯度来更新权值，主要是第一点的问题。最后**随机生成输入权值和输出权值**。这些参数会影响到网络短期记忆时间的长短。输入权值越小而内部矩阵的谱半径越接近1，网络短期记忆时间越长。但是，增强记忆能力的同时，这种操作也造成了网络对“快速变化”系统的建模能力下降。

第二步进行训练。值得注意的是“空转”过程，实际上就是初始化储备池的状态。为什么要进行这个操作呢？因为储备池的内部连接是随机的，最开始的输入序列得到储备池状态的噪声会比较大，所以会先使用一些数据来初始化储备池的状态，从而降低噪声的影响。对于使用线性回归确定输出权值，在下一部分我们来一起进行推导。

## 4、数学推导

我们来对前面所说的输出权值进行求解。问题如下：这里假定对输出权重W做了2范数正则，正则化系数为λ。记网络状态矩阵为X，输出序列矩阵为Y，请写出输出权重W的计算公式。

首先我们要优化的目标是：

<a href="https://www.codecogs.com/eqnedit.php?latex=min\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}&plus;\lambda&space;\left&space;\|&space;W&space;\right&space;\|_{2}^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?min\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}&plus;\lambda&space;\left&space;\|&space;W&space;\right&space;\|_{2}^{2}" title="min\left \| WX-Y \right \|_{2}^{2}+\lambda \left \| W \right \|_{2}^{2}" /></a>

求解非常简单，只需要对其求导，令其导数为0，解出W即可，实际上就是最小二乘法求解，可得：

<a href="https://www.codecogs.com/eqnedit.php?latex=W=YX^{T}(XX^{T}&plus;\lambda&space;I)^{-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W=YX^{T}(XX^{T}&plus;\lambda&space;I)^{-1}" title="W=YX^{T}(XX^{T}+\lambda I)^{-1}" /></a>

进一步说，就是岭回归(带2范数惩罚项的最小二乘回归)。

考虑一个等价的问题：考虑概率情况，<a href="https://www.codecogs.com/eqnedit.php?latex=y(t)=f(t)&plus;\varepsilon" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y(t)=f(t)&plus;\varepsilon" title="y(t)=f(t)+\varepsilon" /></a>,其中<a href="https://www.codecogs.com/eqnedit.php?latex=\varepsilon&space;\sim&space;N(0,\beta^{-1}I)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\varepsilon&space;\sim&space;N(0,\beta^{-1}I)" title="\varepsilon \sim N(0,\beta^{-1}I)" /></a>,

且对于<a href="https://www.codecogs.com/eqnedit.php?latex=W_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{i}" title="W_{i}" /></a>的分布有<a href="https://www.codecogs.com/eqnedit.php?latex=W_{i}\sim&space;N(0,\alpha&space;^{-1}I)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W_{i}\sim&space;N(0,\alpha&space;^{-1}I)" title="W_{i}\sim N(0,\alpha ^{-1}I)" /></a>。

证明最大化后验<a href="https://www.codecogs.com/eqnedit.php?latex=p(W_{i}|X,Y_{i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(W_{i}|X,Y_{i})" title="p(W_{i}|X,Y_{i})" /></a>得到的输出权重与最小二乘法得到的输出权重等价。

最大化后验，即：

<a href="https://www.codecogs.com/eqnedit.php?latex=maxP(W|X,Y)=max\frac{P(W,X,Y)}{P(X,Y)}=max\frac{P(X|W,Y)P(W)}{P(X,Y)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?maxP(W|X,Y)=max\frac{P(W,X,Y)}{P(X,Y)}=max\frac{P(X|W,Y)P(W)}{P(X,Y)}" title="maxP(W|X,Y)=max\frac{P(W,X,Y)}{P(X,Y)}=max\frac{P(X|W,Y)P(W)}{P(X,Y)}" /></a>

由于分母与W无关，所以可以省略，由此上式等价于：

<a href="https://www.codecogs.com/eqnedit.php?latex=maxP(X|W,Y)P(W)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?maxP(X|W,Y)P(W)" title="maxP(X|W,Y)P(W)" /></a>

假设训练样本数为L，则上式等价于：

<a href="https://www.codecogs.com/eqnedit.php?latex=max\prod_{i=1}^{L}P(X_{i}|W_{i},Y)P(W_{i})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?max\prod_{i=1}^{L}P(X_{i}|W_{i},Y)P(W_{i})" title="max\prod_{i=1}^{L}P(X_{i}|W_{i},Y)P(W_{i})" /></a>

加上对数处理后，上式可变为：

<a href="https://www.codecogs.com/eqnedit.php?latex=max\sum_{i=1}^{L}(logP(X_{i}|W_{i},Y_{i})&plus;logP(W_{i}))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?max\sum_{i=1}^{L}(logP(X_{i}|W_{i},Y_{i})&plus;logP(W_{i}))" title="max\sum_{i=1}^{L}(logP(X_{i}|W_{i},Y_{i})+logP(W_{i}))" /></a>

由题目中的条件可知：<a href="https://www.codecogs.com/eqnedit.php?latex=Y_{i}-W_{i}X_{i}\sim&space;N(0,\beta&space;^{-1}I)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y_{i}-W_{i}X_{i}\sim&space;N(0,\beta&space;^{-1}I)" title="Y_{i}-W_{i}X_{i}\sim N(0,\beta ^{-1}I)" /></a>，所以上式可写作：

<a href="https://www.codecogs.com/eqnedit.php?latex=max\sum_{i=1}^{L}(log\frac{1}{\sqrt{2\pi&space;\beta&space;^{-1}I}}exp(-\frac{(Y_{i}-W_{i}X_{i})^{2}}{2&space;\beta&space;^{-1}I})&plus;log\frac{1}{\sqrt{2\pi&space;\alpha^{-1}I}}exp(-\frac{W_{i}^{2}}{2&space;\alpha&space;^{-1}I}))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?max\sum_{i=1}^{L}(log\frac{1}{\sqrt{2\pi&space;\beta&space;^{-1}I}}exp(-\frac{(Y_{i}-W_{i}X_{i})^{2}}{2&space;\beta&space;^{-1}I})&plus;log\frac{1}{\sqrt{2\pi&space;\alpha^{-1}I}}exp(-\frac{W_{i}^{2}}{2&space;\alpha&space;^{-1}I}))" title="max\sum_{i=1}^{L}(log\frac{1}{\sqrt{2\pi \beta ^{-1}I}}exp(-\frac{(Y_{i}-W_{i}X_{i})^{2}}{2 \beta ^{-1}I})+log\frac{1}{\sqrt{2\pi \alpha^{-1}I}}exp(-\frac{W_{i}^{2}}{2 \alpha ^{-1}I}))" /></a>

展开上式，即等价于：

<a href="https://www.codecogs.com/eqnedit.php?latex=max\sum_{i=1}^{L}(-\frac{(Y_{i}-W_{i}X_{i})^{2}}{2&space;\beta&space;^{-1}I}-\frac{W_{i}^{2}}{2&space;\alpha&space;^{-1}I})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?max\sum_{i=1}^{L}(-\frac{(Y_{i}-W_{i}X_{i})^{2}}{2&space;\beta&space;^{-1}I}-\frac{W_{i}^{2}}{2&space;\alpha&space;^{-1}I})" title="max\sum_{i=1}^{L}(-\frac{(Y_{i}-W_{i}X_{i})^{2}}{2 \beta ^{-1}I}-\frac{W_{i}^{2}}{2 \alpha ^{-1}I})" /></a>

将负号去掉，则上式变为：

<a href="https://www.codecogs.com/eqnedit.php?latex=min(\sum_{i=1}^{L}\frac{(Y_{i}-W_{i}X_{i})^{2}}{2&space;\beta&space;^{-1}I}&plus;\sum_{i=1}^{L}\frac{W_{i}^{2}}{2&space;\alpha&space;^{-1}I})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?min(\sum_{i=1}^{L}\frac{(Y_{i}-W_{i}X_{i})^{2}}{2&space;\beta&space;^{-1}I}&plus;\sum_{i=1}^{L}\frac{W_{i}^{2}}{2&space;\alpha&space;^{-1}I})" title="min(\sum_{i=1}^{L}\frac{(Y_{i}-W_{i}X_{i})^{2}}{2 \beta ^{-1}I}+\sum_{i=1}^{L}\frac{W_{i}^{2}}{2 \alpha ^{-1}I})" /></a>

使用矩阵的形式来表示，可得：

<a href="https://www.codecogs.com/eqnedit.php?latex=min\frac{\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}}{2\beta&space;^{-1}I}&plus;\frac{\left&space;\|&space;W&space;\right&space;\|_{2}^{2}}{2\alpha&space;^{-1}I}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?min\frac{\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}}{2\beta&space;^{-1}I}&plus;\frac{\left&space;\|&space;W&space;\right&space;\|_{2}^{2}}{2\alpha&space;^{-1}I}" title="min\frac{\left \| WX-Y \right \|_{2}^{2}}{2\beta ^{-1}I}+\frac{\left \| W \right \|_{2}^{2}}{2\alpha ^{-1}I}" /></a>

进一步地等价于：

<a href="https://www.codecogs.com/eqnedit.php?latex=min\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}&plus;\frac{\alpha&space;}{\beta&space;}\left&space;\|&space;W&space;\right&space;\|_{2}^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?min\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}&plus;\frac{\alpha&space;}{\beta&space;}\left&space;\|&space;W&space;\right&space;\|_{2}^{2}" title="min\left \| WX-Y \right \|_{2}^{2}+\frac{\alpha }{\beta }\left \| W \right \|_{2}^{2}" /></a>

我们令<a href="https://www.codecogs.com/eqnedit.php?latex=\lambda&space;=\frac{\alpha&space;}{\beta&space;}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda&space;=\frac{\alpha&space;}{\beta&space;}" title="\lambda =\frac{\alpha }{\beta }" /></a>，即可得到最终的式子：

<a href="https://www.codecogs.com/eqnedit.php?latex=min\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}&plus;\lambda&space;\left&space;\|&space;W&space;\right&space;\|_{2}^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?min\left&space;\|&space;WX-Y&space;\right&space;\|_{2}^{2}&plus;\lambda&space;\left&space;\|&space;W&space;\right&space;\|_{2}^{2}" title="min\left \| WX-Y \right \|_{2}^{2}+\lambda \left \| W \right \|_{2}^{2}" /></a>



​

## 参考文献

https://blog.csdn.net/minemine999/article/details/80861863

https://www.cnblogs.com/bnuvincent/p/7623318.html

https://blog.csdn.net/qq_42394743/article/details/82082093

https://blog.csdn.net/cassiePython/article/details/80389394