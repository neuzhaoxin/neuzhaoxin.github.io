---
layout: post
title:  "BP神经网络（原理）"
date:   2019-05-15 8:43:06
categories: 机器学习
tags: 神经网络 BPNN
---

* content
{:toc}

主要介绍神经网络的一些基础性知识以及经典BP神经网络的推导过程，包括：什么是人工神经网络、神经元、激活函数、损失函数、反向传播、梯度下降法、整体过程等。





##1、神经网络初探

什么是人工神经网络？首先给出一个经典的定义：“神经网络是由具有适应性的简单单元组成的广泛并行互连网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应”[Kohonen, 1988]。

###1.1神经元模型

神经元模型是模拟生物神经元结构而被设计出来的。神经元大致可以分为树突、突触、细胞体和轴突。树突为神经元的输入通道，其功能是将其它神经元的动作电位传递至细胞体。其它神经元的动作电位借由位于树突分支上的多个突触传递至树突上。神经细胞可以视为有两种状态的机器，激活时为“是”，不激活时为“否”。神经细胞的状态取决于从其他神经细胞接收到的信号量，以及突触的性质（抑制或加强）。当信号量超过某个阈值时，细胞体就会被激活，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。
同理，我们的神经元模型就是为了模拟上述过程，典型的神经元模型如下：

![png](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/BPNN/神经元.png)

（摘自周志华老师《机器学习》第97页）

这个模型中，每个神经元都接受来自其它神经元的输入信号，每个信号都通过一个带有权重的连接传递，神经元把这些信号加起来得到一个总输入值，然后将总输入值与神经元的阈值进行对比（模拟阈值电位），然后通过一个“激活函数”处理得到最终的输出（模拟细胞的激活），这个输出又会作为之后神经元的输入一层一层传递下去。

###1.2激活函数

激活函数的作用就是看该神经细胞是否激活。理想的激活函数是阶跃函数，它将输入值映射为“0”或“1”，“1”表示神经元兴奋，“0”表示神经元抑制。然而，阶跃函数不具备连续性，因此常用Sigmoid函数作为激活函数，它把能在较大范围内变化的输入挤压到（0,1）范围内。

![png](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/BPNN/激活函数.png)

（摘自周志华老师《机器学习》第98页

###1.3神经网络结构

神经网络是一种分层结构，一般由输入层，隐藏层，输出层组成。其中，输入层接收外界输入，隐层和输出层对信号进行加工，最终结果由输出层输出。所以神经网络至少有3层，隐藏层多于1，总层数大于3的就是我们所说的深度学习了。如图为一简单的神经网络：

![JPG](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/BPNN/简单的神经网络图.jpg)

神经网络细胞与细胞之间为连接权重，神经网络学到的东西就蕴含在权值和阈值之中。

##2、正向传播

刚刚讲到神经网络有三层，我们要学习的是细胞间的权值以及隐层和输出层细胞的阈值。正向传播的目的就是从输入层开始，经过隐层到达输出层，得到一个结果，然后看结果合不合适，如果不合适，那么权值和阈值就要改。现在我们来看怎样正向传播。

![png](https://github.com/neuzhaoxin/neuzhaoxin.github.io/raw/master/_posts/pictures/BPNN/神经网络.png)

（摘自周志华老师《机器学习》第102页

我们先规定一些符号。

输入向量：

<a href="https://www.codecogs.com/eqnedit.php?latex=\vec{x}=\left&space;\{&space;x_{1},x_{2},...x_{i},...,x_{d}&space;\right&space;\},i=1,2,...,d" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vec{x}=\left&space;\{&space;x_{1},x_{2},...x_{i},...,x_{d}&space;\right&space;\},i=1,2,...,d" title="\vec{x}=\left \{ x_{1},x_{2},...x_{i},...,x_{d} \right \},i=1,2,...,d" /></a>


输出向量：

<a href="https://www.codecogs.com/eqnedit.php?latex=\vec{y}=\left&space;\{&space;y_{1},y_{2},...y_{j},...,y_{l}&space;\right&space;\},j=1,2,...,l" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vec{y}=\left&space;\{&space;y_{1},y_{2},...y_{j},...,y_{l}&space;\right&space;\},j=1,2,...,l" title="\vec{y}=\left \{ y_{1},y_{2},...y_{j},...,y_{l} \right \},j=1,2,...,l" /></a>

隐层神经元输出向量：

<a href="https://www.codecogs.com/eqnedit.php?latex=\vec{b}=\left&space;\{&space;b_{1},b_{2},...b_{h},...,y_{q}&space;\right&space;\},h=1,2,...,q" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\vec{b}=\left&space;\{&space;b_{1},b_{2},...b_{h},...,y_{q}&space;\right&space;\},h=1,2,...,q" title="\vec{b}=\left \{ b_{1},b_{2},...b_{h},...,y_{q} \right \},h=1,2,...,q" /></a>

第i个输入层到第h个隐层的权值：<a href="https://www.codecogs.com/eqnedit.php?latex=v_{ih}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{ih}" title="v_{ih}" /></a>

第h个隐层到第j个输出层的权值：<a href="https://www.codecogs.com/eqnedit.php?latex=w_{hj}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_{hj}" title="w_{hj}" /></a>

由图可知，第h个隐层神经元的输入为：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=a_{h}=\sum_{i=1}^{d}v_{ih}x_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a_{h}=\sum_{i=1}^{d}v_{ih}x_{i}" title="a_{h}=\sum_{i=1}^{d}v_{ih}x_{i}" /></a>　　　　（公式1）

第h个隐层神经元的输出为（激活函数用sigmoid函数，以f()表示）：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=b_{h}=f(a_{h}-&space;\gamma&space;_{h})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?b_{h}=f(a_{h}-&space;\gamma&space;_{h})" title="b_{h}=f(a_{h}- \gamma _{h})" /></a>　　　　（公式2）

其中<a href="https://www.codecogs.com/eqnedit.php?latex=\gamma&space;_{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\gamma&space;_{h}" title="\gamma _{h}" /></a>为隐层阈值。

第j个输出神经元的输入为：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\beta&space;_{j}=\sum_{h=1}^{q}w_{hj}b_{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\beta&space;_{j}=\sum_{h=1}^{q}w_{hj}b_{h}" title="\beta _{j}=\sum_{h=1}^{q}w_{hj}b_{h}" /></a>　　　　（公式3）

神经网络最终的预测输出为（激活函数用sigmoid函数，以f()表示）：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{y_{j}}=f(\beta&space;_{j}-\theta&space;_{j})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{y_{j}}=f(\beta&space;_{j}-\theta&space;_{j})" title="\widehat{y_{j}}=f(\beta _{j}-\theta _{j})" /></a>　　　　（公式4）

其中<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;_{j}" title="\theta _{j}" /></a>为输出层的阈值。

至此，输出结果已经有了，下面该看结果合不合适了。我们常用的方法是计算预测输出<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{y_{j}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{y_{j}}" title="\widehat{y_{j}}" /></a>与实际输出<a href="https://www.codecogs.com/eqnedit.php?latex=y_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_{j}" title="y_{j}" /></a>的均方误差，即：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=E=\tfrac{1}{2}\sum_{i=1}^{j}(\widehat{y_{j}}-y_{j})^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E=\tfrac{1}{2}\sum_{i=1}^{j}(\widehat{y_{j}}-y_{j})^{2}" title="E=\tfrac{1}{2}\sum_{i=1}^{j}(\widehat{y_{j}}-y_{j})^{2}" /></a>　　　　　（公式5）

这个函数前面的系数只是为了之后求导方便，不影响函数的性质。

只要这个均方误差随着我们权值和阈值的改变不断减小，那我们的学习就是成功的。到底如何变小呢？这就用到了梯度下降策略，将误差进行反向传播。

##3、反向传播

均方误差（损失函数）必定存在一个最小值，我们的目的就是寻找这个最小值。梯度是一个利用求导得到的数值，可以理解为参数的变化量。从几何意义上来看，梯度代表一个损失函数增加最快的方向，反之，沿着相反的方向就可以不断地使损失逼近最小值，也就是使网络逼近真实的关系，这就是梯度下降法。

那么反向传播的过程就可以理解为，根据损失函数，来反向计算出每个参数（如
<a href="https://www.codecogs.com/eqnedit.php?latex=v_{ih}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{ih}" title="v_{ih}" /></a>
,
<a href="https://www.codecogs.com/eqnedit.php?latex=w_{hj}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_{hj}" title="w_{hj}" /></a>
，
<a href="https://www.codecogs.com/eqnedit.php?latex=\gamma&space;_{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\gamma&space;_{h}" title="\gamma _{h}" /></a>
，
<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;_{j}" title="\theta _{j}" /></a>
等）的梯度
<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;v_{ih}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;v_{ih}" title="\Delta v_{ih}" /></a>
，
<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_{hj}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;w_{hj}" title="\Delta w_{hj}" /></a>
，
<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\gamma&space;_{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\gamma&space;_{h}" title="\Delta \gamma _{h}" /></a>
，
<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\theta&space;_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\theta&space;_{j}" title="\Delta \theta _{j}" /></a>
等等，再将原来的参数分别加上自己对应的梯度，就完成了一次反向传播。

以
<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_{hj}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;w_{hj}" title="\Delta w_{hj}" /></a>
为例介绍一下计算过程。
对于（公式5）第Ｋ组训练集的误差
<a href="https://www.codecogs.com/eqnedit.php?latex=E_{k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E_{k}" title="E_{k}" /></a>
，给定学习率
<a href="https://www.codecogs.com/eqnedit.php?latex=\eta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\eta" title="\eta" /></a>
，有：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_{hj}=-\eta&space;\frac{\partial&space;E_{k}}{\partial&space;w_{hj}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;w_{hj}=-\eta&space;\frac{\partial&space;E_{k}}{\partial&space;w_{hj}}" title="\Delta w_{hj}=-\eta \frac{\partial E_{k}}{\partial w_{hj}}" /></a>　　　　（公式6）

又因为
<a href="https://www.codecogs.com/eqnedit.php?latex=w_{hj}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_{hj}" title="w_{hj}" /></a>
先影响到
<a href="https://www.codecogs.com/eqnedit.php?latex=\beta&space;_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\beta&space;_{j}" title="\beta _{j}" /></a>
，再影响到
<a href="https://www.codecogs.com/eqnedit.php?latex=\widehat{y}_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widehat{y}_{j}" title="\widehat{y}_{j}" /></a>
，最后影响到
<a href="https://www.codecogs.com/eqnedit.php?latex=E_{k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E_{k}" title="E_{k}" /></a>
，因此：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;E_{k}}{\partial&space;w_{hj}}=\frac{\partial&space;E_{k}}{\partial&space;\widehat{y}_{j}}\cdot&space;\frac{\partial&space;\widehat{y}_{j}}{\partial&space;\beta&space;_{j}}\cdot&space;\frac{\partial&space;\beta&space;_{j}}{\partial&space;w_{hj}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;E_{k}}{\partial&space;w_{hj}}=\frac{\partial&space;E_{k}}{\partial&space;\widehat{y}_{j}}\cdot&space;\frac{\partial&space;\widehat{y}_{j}}{\partial&space;\beta&space;_{j}}\cdot&space;\frac{\partial&space;\beta&space;_{j}}{\partial&space;w_{hj}}" title="\frac{\partial E_{k}}{\partial w_{hj}}=\frac{\partial E_{k}}{\partial \widehat{y}_{j}}\cdot \frac{\partial \widehat{y}_{j}}{\partial \beta _{j}}\cdot \frac{\partial \beta _{j}}{\partial w_{hj}}" /></a>　　　　（公式7）

只要将该公式每一项计算出来就可以了。

根据公式（3），

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;\beta&space;_{j}}{\partial&space;w_{hj}}=b_{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;\beta&space;_{j}}{\partial&space;w_{hj}}=b_{h}" title="\frac{\partial \beta _{j}}{\partial w_{hj}}=b_{h}" /></a>　　　　（公式8）

根据sigmoid函数的性质

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=f'(x)=f(x)(1-f(x))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f'(x)=f(x)(1-f(x))" title="f'(x)=f(x)(1-f(x))" /></a>　　　　（公式9）

再依据（公式5）（公式4）

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=-\frac{\partial&space;E_{k}}{\partial&space;\widehat{y}_{j}}\cdot&space;\frac{\partial&space;\widehat{y}_{j}}{\partial&space;\beta&space;_{j}}&space;=-(\widehat{y}_{j}^{k}-y_{j}^{k})f'(\beta&space;_{j}-\theta&space;_{j})&space;=\widehat{y}_{j}^{k}(1-\widehat{y}_{j}^{k})(y_{j}^{k}-\widehat{y}_{j}^{k})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?-\frac{\partial&space;E_{k}}{\partial&space;\widehat{y}_{j}}\cdot&space;\frac{\partial&space;\widehat{y}_{j}}{\partial&space;\beta&space;_{j}}&space;=-(\widehat{y}_{j}^{k}-y_{j}^{k})f'(\beta&space;_{j}-\theta&space;_{j})&space;=\widehat{y}_{j}^{k}(1-\widehat{y}_{j}^{k})(y_{j}^{k}-\widehat{y}_{j}^{k})" title="-\frac{\partial E_{k}}{\partial \widehat{y}_{j}}\cdot \frac{\partial \widehat{y}_{j}}{\partial \beta _{j}} =-(\widehat{y}_{j}^{k}-y_{j}^{k})f'(\beta _{j}-\theta _{j}) =\widehat{y}_{j}^{k}(1-\widehat{y}_{j}^{k})(y_{j}^{k}-\widehat{y}_{j}^{k})" /></a>　　　　（公式10）

最终得到（公式6）的结果

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;w_{hj}&space;=\eta&space;\cdot&space;\widehat{y}_{j}^{k}(1-\widehat{y}_{j}^{k})(y_{j}^{k}-\widehat{y}_{j}^{k})\cdot&space;b_{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;w_{hj}&space;=\eta&space;\cdot&space;\widehat{y}_{j}^{k}(1-\widehat{y}_{j}^{k})(y_{j}^{k}-\widehat{y}_{j}^{k})\cdot&space;b_{h}" title="\Delta w_{hj} =\eta \cdot \widehat{y}_{j}^{k}(1-\widehat{y}_{j}^{k})(y_{j}^{k}-\widehat{y}_{j}^{k})\cdot b_{h}" /></a>　　　　（公式11）

同理也可以得到以下公式：

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\theta&space;_{j}&space;=-\eta&space;\cdot&space;g_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\theta&space;_{j}&space;=-\eta&space;\cdot&space;g_{j}" title="\Delta \theta _{j} =-\eta \cdot g_{j}" /></a>　　　　（公式12）

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;v_{ih}&space;=\eta&space;\cdot&space;e_{h}\cdot&space;x_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;v_{ih}&space;=\eta&space;\cdot&space;e_{h}\cdot&space;x_{i}" title="\Delta v_{ih} =\eta \cdot e_{h}\cdot x_{i}" /></a>　　　　（公式13）

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;\gamma&space;_{h}&space;=-\eta&space;\cdot&space;e_{h}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;\gamma&space;_{h}&space;=-\eta&space;\cdot&space;e_{h}" title="\Delta \gamma _{h} =-\eta \cdot e_{h}" /></a>　　　　（公式14）

其中，

　　　　　　　　<a href="https://www.codecogs.com/eqnedit.php?latex=e_{h}=-\frac{\partial&space;E_{k}}{\partial&space;b_{h}}\cdot&space;\frac{\partial&space;b_{h}}{\partial&space;\alpha&space;_{h}}=-\sum_{j=1}^{l}\frac{\partial&space;E_{k}}{\partial&space;\beta_{j}}\cdot&space;\frac{\partial&space;\beta&space;_{j}}{\partial&space;b_{h}}f'(\alpha&space;_{h}-\gamma&space;_{h})=\sum_{j=1}^{l}w_{hj}g_{j}f'(\alpha&space;_{h}-\gamma&space;_{h})=b_{h}(1-b_{h})\sum_{j=1}^{l}w_{hj}g_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?e_{h}=-\frac{\partial&space;E_{k}}{\partial&space;b_{h}}\cdot&space;\frac{\partial&space;b_{h}}{\partial&space;\alpha&space;_{h}}=-\sum_{j=1}^{l}\frac{\partial&space;E_{k}}{\partial&space;\beta_{j}}\cdot&space;\frac{\partial&space;\beta&space;_{j}}{\partial&space;b_{h}}f'(\alpha&space;_{h}-\gamma&space;_{h})=\sum_{j=1}^{l}w_{hj}g_{j}f'(\alpha&space;_{h}-\gamma&space;_{h})=b_{h}(1-b_{h})\sum_{j=1}^{l}w_{hj}g_{j}" title="e_{h}=-\frac{\partial E_{k}}{\partial b_{h}}\cdot \frac{\partial b_{h}}{\partial \alpha _{h}}=-\sum_{j=1}^{l}\frac{\partial E_{k}}{\partial \beta_{j}}\cdot \frac{\partial \beta _{j}}{\partial b_{h}}f'(\alpha _{h}-\gamma _{h})=\sum_{j=1}^{l}w_{hj}g_{j}f'(\alpha _{h}-\gamma _{h})=b_{h}(1-b_{h})\sum_{j=1}^{l}w_{hj}g_{j}" /></a>　　　　（公式15）

按照计算得出的变化量改变权值和阈值，继续进行学习，直到均方误差达到要求。




##参考文献：

周志华老师《机器学习》

https://blog.csdn.net/u014303046/article/details/78200010

https://blog.csdn.net/daaikuaichuan/article/details/81135802

https://www.cnblogs.com/biaoyu/p/4591304.html

https://blog.csdn.net/weixin_40432828/article/details/82192709




注：转载请注明，来自https://neuzhaoxin.github.io 
